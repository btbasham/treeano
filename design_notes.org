* general
- architecture as data
  - the architecture of a single model should at every point be serializable and deserializable to the same model
- parameterization should be recursive
  - eg inner parameterization inherit outer parameterization unless explicitly overwritten
    - maybe: parameterization such as update strategy and initialization
  - use this for things like dropout
- shape must be knowable statically, at least for a fraction of the network
  - before learned weight initialization
    - eg. so that fully connected layers know their size
  - not all layers require shape
    - eg. conv/maxpool/spatial pyramid pooling
    - eg. if there is a spatial pyramid pooling layer, don't need to know shapes of conv layers
- all nodes should behave like data and only data
  - the interface should be what data it contains
  - possibly lazy data, though
- serialization
  - model architecture should be serialized in human-readable / edittable format
    - eg. yaml
    - being able to be serialized to json is probably a pretty safe format
      - though not necessarily serialized to json
  - model weights should be in some agnostic key-value store format
    - a similar architecture should be able to read in weights perfectly
    - path -> np.array
  - should be possible to share weights via specifying a similar architecture
    - since the layer names are absolutely critical to what "similar" means, it seems safe to assume that this sharing can come of the form of converting the other net into this key->value form and having the new net simply read from this blob
  - all layers can specify how they should be serialized (as data)
- input parameters
  - assumption: local networks (local = created together / close together) will have similar parameters
  - thus we can have local "inheritance" of parameters
    - eg. outer layer has an immutable dict of parameters, inner layer overwrites them when it has a parameter override, more inner layers can read from parameters with a key
    - this requires non-ambiguous keys
      - ie. 2 layers shouldn't have the same key mean different things
- need metadata on weight parameters
  - still need to specify backprop-able params, since the gradient needs to be calculated
  - still need to specify non-bias params, for regularization?
    - not necessarily, if the layer knows how to handle it's own parameters
    - this makes it easier if there is a large number of layers expected to be handle the same way
    - "the expression problem"
      - you might add a new layer that would want to benefit from all the regularization
      - you might add new regularization that would wnat to benefit from all layers
      - what commonality can we extract from this?
        - different parameters can behave the same way
        - eg. bias / inputweights
          - makes sense to have an "escape hatch" where things can work if not fitting within a mold
  - parameters should have tagging
    - to work with multiple kinds of regularization
- question: should all nodes have a unique name?
  - pros of unique name
    - don't need separate concepts for unique / non-unique
      - and some names do need to be unique (for initialization between networks)
  - cons
    - conceptually unnecessary, as long as they have a unique path
      - eg. don't need to name "convpoolstack_pool" when "pool" has parent "convpoolstack"
        - this would also be DRY-er and make the architectures more human readable
  - conclusion: just go with everything having a unique name
    - this should be easier to understand - and the downsides aren't that bad
- question: should hyperparameters have global default values
  - eg. shared initialization (create the var)
  - eg. weight initialization (0s)
  - answer: see "question: should parameters and shared variables have different initializations?"
- question: should parameters and shared variables have different initializations?
  - pros of different:
    - they are 2 very different things, no reason to have one know about the other
      - eg. Uniform initialization shouldn't have to know how to create a shared variable
    - if they were the same, and there was a subnetwork with different initialization, that subnetwork couldn't be initialized with shared variables from another network
      - this means that there must be a mechanism by which the shared initialization can overwrite that initialization
  - cons of different:
    - this is a purely theano-level concern, thus a conceptual network shouldn't require it
    - when using shared variables from a different network, you probably don't want to re-initialize
  - conclusion: should be different - and a weight initialization node should look up the tree for a hyperparameter on shared initialization
    - and the weight initialization should have a default shared initialization
      - thus no global shared initialization should be needed
- should lifecycle methods (init_state, init_shared, init_values, and compute) be separate?
  - pros of separation
    - easier to do special schemes like loading existing network
    - may allow input layers to be partially initialized before initializing
  - cons
    - more boilerplate
    - somewhat more elegant to define everything in a single function
      - might not even need laziness
  - misc
    - seems unlikely that there would be a good use for having partially initialized layers
  - thought: init_state is different from the others - since initializing the variables can be lazy
    - alternative: pre_compute and post_compute steps for pre-order and post-order traversal
    - pros of separating only init_state
      - if they happen all at once, some nodes would need a pre-order traversal (eg. an initialization node that creates a stateful initializer), while pretty much all nodes need a post-order traversal (to specify output)
  - conclusion:
    - separate init_state, but have a single compute step which:
      - creates shared variables (parameters)
      - creates outputs
      - sets initial values of shared variables (parameters)
- using networkx to represent the graph
  - pros
    - a bunch of stuff for free
      - make it easy to look for ancestors, etc.
      - topological sort
      - make cute diagram of an architecture
  - cons
    - extra dependency
- should architecture nodes be separate from state nodes?
  - they conceptually serve 2 very different purposes
  - but there implementations are linked together
    - ie. the data in an architecture node is used for a state node
  - separating them would require additional verbosity to combine them together
  - conclusion: keep them together, but make sure not to mutate them
- question: where are dependencies defined?
  - NOTE: only containers need to define dependencies
  - question: how to define a node which is shared between multiple trees (eg. the same thing is processed in 2 places)
    - just because something is build like a tree, doesn't mean there can't be jumps in it
      - can have a "get_layer_with_name" node
  - conclusion:
    - parents automatically depend on their children
    - a "get_layer_with_name" node depends on that layer
    - more dependencies can be defined within nodes
- question: should we be able to add nodes in the graph after build-ing?
  - eg. add special intermediate nodes
  - use case: container node wants to create some sort of state that it's children can depend on
  - conclusion: no
    - one would have to make sure it's initialized just like in build
    - this kind of change seems like it would make it easy to violate assumptions / principle of least surprise
- how to do composite node dependencies properly
  - ie. how to pass input of sequential node to first node
  - how do you have it return the output of its last child? (sequential node)
    - the input of a composite node will be set by its parent - thus it can't set it's own dependencies on its children
  - how to express dependencies of first child:
    - first child depends on input of sequential node
    - last child depends on other children
    - sequential node output depends on last child
    - if first child depends on sequential node, this causes a cycle
  - there is a true dependency between the input of the sequential node and its first child
    - thus it makes sense to make this explicit
  - conclusion:
    - walk tree from top-level to bottom level, so that a dependency between the input of the sequential node and the first child can be set
  - alternative:
    - some form of graph unification
      - eg.
        - unify inputs of sequential node with inputs of first child
        - unify outputs of sequential node with outputs for last child
- question: how are inputs passed into a node?
  - parent node should handle it
- question: how should the input to a node be passed in?
  - options
    - function argument
      - pros
        - more expected
        - can enfore the right arity
      - cons
        - can't specify which key in the output map
    - through the graph
      - eg. self.graph.my_input(some_key="output")
    - through the graph w/ sugar
      - eg. self.get_input(some_key="output")
  - conclusion:
    - through the graph w/ sugar
    - no positional arguments (much harder to specify in a graph)
- why have wrapped variables?
  - additonal metadata
    - eg. shape, tags
  - custom rules
    - eg. related to tags
      - shared variables should be parameters and either weight / bias
      - weight / bias -> also parameter
    - eg. parameters must be shared
  - why make them lazy?
    - original motivation: to allow stage-wise initialization
      - eg.
        - first create the shareds
        - then perform operation with them
        - then adds some values in
  - pros of unwrapped variables
    - can directly look at the computation graph for dependencies
  - solution:
    - include a reference to the wrapped variable in the theano variable
      - so that the computation graph can still be walked for dependencies
* best practices
- store all constructor arguments as is
  - don't mutate them
  - don't overwrite them
  - don't forget to store them as an attribute
  - rationale: this allows serialization of the original layer
- use https://github.com/ionelmc/python-fields for constructing nodes
  - helps follow good constructor practices, and provides a nice string representation
* assumptions
- parameter tensors will have a fixed size
- nodes are in a directed acyclic graph
  - thus can be traversed in a topological sort
- initialization schemes can work in a topologically sorted order
  - eg. not from output to input
  - not necessarily true, but a simplifying assumption
- parameter tensors are owned by a single node in the computation graph, and that node is responsible for their updates
* random notes
- model everything as a "layer"
  - including loss/objective
  - maybe even represent gradient descent updater as layer node
    - takes in trees of things to update, and loss, and generates updates
  - layer : higher level unit than theano node
    - high level enough that it deserves to both be know and named
  - compose base layers together with functions
    - make sure that functions add a named identity layer
  - use paths instead of strings as names
- issue: how to update an existing architecture to eg. use dropout
  - have a top level assoc_in
    - to add new parameters or replace layers
- responsibilities of each "layer"
  - serialization
  - deserialization
  - output(s)
    - default key for output: "output"
    - map w/ names
    - rationale for multiple outputs:
      - monitoring is one possible use case
  - shape(s)
    - optional: can auto-compute
  - update(s)
    - optional
  - dimension(s)
    - maybe?
- everything has names and paths
  - each node needs a name
  - names are like relative paths
  - paths are absolute in the network
    - tuples of names / indexes
- sharing shared: separate step for initializing shared
  - pass in a map from path to shared, which can be used instead of creating a new shared variable
- sharing weights: separate step for initializing weights
  - pass in a map from path to weights, which can be used instead of creating a new shared variable
- sharing params:
  - use a network.assoc_in(path, value)
    - eg. top_level_network.assoc_in(["deterministic"], True)
- question: when are shareds / weights initialized
  - initializing shared
    - prereq: dimensionality, broadcast dims
  - initializing weights
    - prereq: shape
- state in a node is divide into:
  - hyperparameters
  - shared variables
  - intermediates
- how to do shared initialization:
  - kinds of shared initialization
    - exact path initialization
      - eg. ("FCs", "head", 1, "FC")
    - partial path initialization
      - eg. ("some_initalization", "something", "my_cool_layer", "FC") equivalent to ("my_cool_layer", "FC")
  - question: conceptually, how does one perform initialization with existing shareds
    - SharedInitialization.fromNode(other_network, backup_initialization=NoInitialization())
      - set default to having no init
* TODOs
- test with some identity nodes / input nodes
- create hyperparameter propagation
  - test w/ multiply by constant nodes
- custom wrapper around variables
  - if shape is not given, print warning and calculate
    - theano.function([x, y], [x], on_unused_input="ignore")(2, 3)
- create Node.function() as syntactic sugar for creating theano.function's
- create parameter searching
- create updating
  - handle generate_updates in function
  - represent updates as deltas, not new values
    - rationale: can do interesting things like add them together, or multiply by -1 to train adversarially
  - update delta container
    - eg.
      - UpdateDeltas.apply(lambda x: x * 2)
        - multiplies each update by 2
- fix architecture to data / from data / copy
  - problem: what if node has children in its state
    - some sort of walk needs to occur converting everything into data and back
- wrap lasagne
- create graph transformations
  - eg.
    - substitution
      - eg. replace ReLU -> PReLU / leaky ReLU
        - use an update-in operation on the architecture as data that does a tree walk
      - to substitute in only a subtree
    - adding different initialization
- break up into different modules
- create pretty printing of nodes
  - like how theano prints it's computation graph
- demos
  - vanilla mnist
  - fully connected layer mnist with num_hidden, activation specified once
  - separate conv layers from FC layers
    - different dropout and L2 for conv
- use class decorator for registering classes for serialization
  - look into using a metaclass in the future
- consider as a name: treehano
  - pros
    - sounds cooler
  - cons
    - sounds less like theano
- issue: recurrent nets
  - if everything is kept as a function from parameters -> theano var, it should compose well with recurrent nets
  - recurrent layers would probably behave something like a multiple input layer
  - if a net is recurrent, this seems like it would just take an orthogonal compilation step to convert the resulting net to use scan
- list of possible variable tags:
  - input
  - weight
  - bias
  - parameter
  - monitor
  - state
    - eg. batch normalization state
