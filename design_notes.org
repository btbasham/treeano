* general
- architecture as data
  - the architecture of a single model should at every point be serializable and deserializable to the same model
- parameterization should be recursive
  - eg inner parameterization inherit outer parameterization unless explicitly overwritten
    - maybe: parameterization such as update strategy and initialization
  - use this for things like dropout
- shape must be knowable statically, at least for a fraction of the network
  - before learned weight initialization
    - eg. so that fully connected layers know their size
  - not all layers require shape
    - eg. conv/maxpool/spatial pyramid pooling
    - eg. if there is a spatial pyramid pooling layer, don't need to know shapes of conv layers
- all nodes should behave like data and only data
  - the interface should be what data it contains
  - possibly lazy data, though
- serialization
  - model architecture should be serialized in human-readable / edittable format
    - eg. yaml
    - being able to be serialized to json is probably a pretty safe format
      - though not necessarily serialized to json
  - model weights should be in some agnostic key-value store format
    - a similar architecture should be able to read in weights perfectly
    - path -> np.array
  - should be possible to share weights via specifying a similar architecture
    - since the layer names are absolutely critical to what "similar" means, it seems safe to assume that this sharing can come of the form of converting the other net into this key->value form and having the new net simply read from this blob
  - all layers can specify how they should be serialized (as data)
- input parameters
  - assumption: local networks (local = created together / close together) will have similar parameters
  - thus we can have local "inheritance" of parameters
    - eg. outer layer has an immutable dict of parameters, inner layer overwrites them when it has a parameter override, more inner layers can read from parameters with a key
    - this requires non-ambiguous keys
      - ie. 2 layers shouldn't have the same key mean different things
- need metadata on weight parameters
  - still need to specify backprop-able params, since the gradient needs to be calculated
  - still need to specify non-bias params, for regularization?
    - not necessarily, if the layer knows how to handle it's own parameters
    - this makes it easier if there is a large number of layers expected to be handle the same way
    - "the expression problem"
      - you might add a new layer that would want to benefit from all the regularization
      - you might add new regularization that would wnat to benefit from all layers
      - what commonality can we extract from this?
        - different parameters can behave the same way
        - eg. bias / inputweights
          - makes sense to have an "escape hatch" where things can work if not fitting within a mold
  - parameters should have tagging
    - to work with multiple kinds of regularization
- question: should all nodes have a unique name?
  - pros of unique name
    - don't need separate concepts for unique / non-unique
      - and some names do need to be unique (for initialization between networks)
  - cons
    - conceptually unnecessary, as long as they have a unique path
      - eg. don't need to name "convpoolstack_pool" when "pool" has parent "convpoolstack"
        - this would also be DRY-er and make the architectures more human readable
  - conclusion: just go with everything having a unique name
    - this should be easier to understand - and the downsides aren't that bad
- question: should hyperparameters have global default values
  - eg. shared initialization (create the var)
  - eg. weight initialization (0s)
  - answer: see "question: should parameters and shared variables have different initializations?"
- question: should parameters and shared variables have different initializations?
  - pros of different:
    - they are 2 very different things, no reason to have one know about the other
      - eg. Uniform initialization shouldn't have to know how to create a shared variable
    - if they were the same, and there was a subnetwork with different initialization, that subnetwork couldn't be initialized with shared variables from another network
      - this means that there must be a mechanism by which the shared initialization can overwrite that initialization
  - cons of different:
    - this is a purely theano-level concern, thus a conceptual network shouldn't require it
    - when using shared variables from a different network, you probably don't want to re-initialize
  - conclusion:
    - should be different
      - and a weight initialization node should look up the tree for a hyperparameter on shared initialization
    - weight initialization should have a default shared initialization
      - thus no global shared initialization should be needed
    - weight initialization and shared variable creation should happen at the same time, so that weight initialization knows now to overwrite the values in a shared variable
- should lifecycle methods (init_state, init_shared, init_values, and compute) be separate?
  - pros of separation
    - easier to do special schemes like loading existing network
    - may allow input layers to be partially initialized before initializing
  - cons
    - more boilerplate
    - somewhat more elegant to define everything in a single function
      - might not even need laziness
  - misc
    - seems unlikely that there would be a good use for having partially initialized layers
  - thought: init_state is different from the others - since initializing the variables can be lazy
    - alternative: pre_compute and post_compute steps for pre-order and post-order traversal
    - pros of separating only init_state
      - if they happen all at once, some nodes would need a pre-order traversal (eg. an initialization node that creates a stateful initializer), while pretty much all nodes need a post-order traversal (to specify output)
  - conclusion:
    - separate init_state, but have a single compute step which:
      - creates shared variables (parameters)
      - creates outputs
      - sets initial values of shared variables (parameters)
- using networkx to represent the graph
  - pros
    - a bunch of stuff for free
      - make it easy to look for ancestors, etc.
      - topological sort
      - make cute diagram of an architecture
  - cons
    - extra dependency
- should architecture nodes be separate from state nodes?
  - they conceptually serve 2 very different purposes
  - but there implementations are linked together
    - ie. the data in an architecture node is used for a state node
  - separating them would require additional verbosity to combine them together
  - conclusion: keep them together, but make sure not to mutate them
- question: where are dependencies defined?
  - NOTE: only containers need to define dependencies
  - question: how to define a node which is shared between multiple trees (eg. the same thing is processed in 2 places)
    - just because something is build like a tree, doesn't mean there can't be jumps in it
      - can have a "get_layer_with_name" node
  - conclusion:
    - parents automatically depend on their children
    - a "get_layer_with_name" node depends on that layer
    - more dependencies can be defined within nodes
- question: should we be able to add nodes in the graph after build-ing?
  - eg. add special intermediate nodes
  - use case: container node wants to create some sort of state that it's children can depend on
  - conclusion: no
    - one would have to make sure it's initialized just like in build
    - this kind of change seems like it would make it easy to violate assumptions / principle of least surprise
- how to do composite node dependencies properly
  - ie. how to pass input of sequential node to first node
  - how do you have it return the output of its last child? (sequential node)
    - the input of a composite node will be set by its parent - thus it can't set it's own dependencies on its children
  - how to express dependencies of first child:
    - first child depends on input of sequential node
    - last child depends on other children
    - sequential node output depends on last child
    - if first child depends on sequential node, this causes a cycle
  - there is a true dependency between the input of the sequential node and its first child
    - thus it makes sense to make this explicit
  - conclusion:
    - walk tree from top-level to bottom level, so that a dependency between the input of the sequential node and the first child can be set
  - alternative:
    - some form of graph unification
      - eg.
        - unify inputs of sequential node with inputs of first child
        - unify outputs of sequential node with outputs for last child
- question: how are inputs passed into a node?
  - parent node should handle it
- question: how should the input to a node be passed in?
  - options
    - function argument
      - pros
        - more expected
        - can enfore the right arity
      - cons
        - can't specify which key in the output map
    - through the graph
      - eg. self.graph.my_input(some_key="output")
    - through the graph w/ sugar
      - eg. self.get_input(some_key="output")
  - conclusion:
    - through the graph w/ sugar
    - no positional arguments (much harder to specify in a graph)
- why have wrapped variables?
  - additonal metadata
    - eg. shape, tags
  - custom rules
    - eg. related to tags
      - shared variables should be parameters and either weight / bias
      - weight / bias -> also parameter
    - eg. parameters must be shared
  - why make them lazy?
    - original motivation: to allow stage-wise initialization
      - eg.
        - first create the shareds
        - then perform operation with them
        - then adds some values in
  - pros of unwrapped variables
    - can directly look at the computation graph for dependencies
  - solution:
    - include a reference to the wrapped variable in the theano variable
      - so that the computation graph can still be walked for dependencies
- why not using tagging on theano variables?
  - because theano variables can be shared between networks
    - the state of one network shouldn't be messed with the state of another
- question: should nodes update themselves or should a higher level node do it?
  - pros if nodes updates themselves:
    - nodes can make sure each update only occurs once
  - pros of higher level node doing it:
    - code might be cleaner - because it can update all the weights in a single batch
  - question: is there any way to set higher level nodes to only update the weight of a unit once?
    - or can a lower level node override those updates
  - conclusion: higher level node can do it, lower level nodes can override
- how to compute update deltas?
  - compute bottom-up in architectural tree
    - pros:
      - can be done by adding immutable UpdateDeltas together
  - compute top-down in architectural tree
    - so children can overwrite / add
    - pros:
      - more complex logic can happen in a single mode
    - cons:
      - if using a single mutable UpdateDeltas, then inner nodes can see state of nodes that aren't its parents
        - eg.
          - b<-x->a (x w/ children a,b)
          - x computes its updates
          - a can see x's updates
          - thus a can also see updates for node b from x
          - whichever of a,b is computed second will see updates from the other
        - this may cause errors as well if one accidentaly updates weights that they shouldn't be able to see
  - add it all together
    - must be equivalent to an implicit top-down
      - eg. in order for a node to zero out updates to its children, some sort of top-down process must occur
    - pros
      - simpler interface
    - cons
      - complexity must be implemented anyway - why not expose it
  - conclusion: top-down
    - rationale:
      - lower in the tree is more specific, and you'd more likely want to perform an effect on a general setting than apply a more general effect to independent local settings
        - eg. if you want to zero out updates, more likely a non-local update than lots of independent local updates
- how should updatedeltas be updated (assuming top-down / root-to-leaf computation)?
  - it seems that either implicitly or explicitly, update deltas need to be passed into nodes
    - how else would a node zero out the updates?
      - doing so declaratively is a (complex) possibility
        - especially a simplified version with only the ability to add / overwrite
  - options
    - update_deltas should be passed in to a compute_update_deltas function
      - pros
        - consistent with normal python functions
    - update_deltas should be accessed as a attribute of the current node
      - pros
        - makes it clear where they are stored, always being in the same place
        - they should be accessed as an attribute in the future
  - conclusion:
    - update_deltas should be shared for a network
      - because
        - a single network should have a single set of updates
        - every node gets access to the updates anyway via compute_update_deltas
        - the updates should be stored somewhere
          - since they don't change, they can be cached
        - instead of storing then in some arbitrary node, we can store them in all nodes
    - update_deltas should be passed in to a compute_update_deltas function
      - rationale: simpler, makes sense, and might be useful for re-applying/re-ordering updates
      - note: should assert that compute_update_deltas returns None to make the interface clearly be one for mutation
    - because update deltas will be shared and mutable, their api should be made such that it isn't ease to accidentally mess than up
      - eg. use __iadd__ instead of __add__
- should computing output and update deltas happen at the same time?
  - no, because some node's updates will require outputs computed later in the DAG
    - eg. recurrent state would naturally have a loop
- the input arguments for computing the output of a node should be a function of the network
  - for nodes that dynamically get new input keys
    - eg. a node that is sent input
      - eg. a cost node that all costs are sent to
- node API
  - architecture nodes ONLY have 2 things: hyperparameters and children
    - in various desired formats
      - might want custom ways of inputting children
        - eg. networkx DAG
      - might want custom ways of inputting hyperparameters
        - eg. hyperparameter node
          - takes in kwargs
        - eg. something that provides stateful hyperparameters
    - how to effectively define these things in an easy to use way?
      - to minimize overall work + maximize simplicity
        - specifically in:
          - __init__
          - getting hyperparameters
          - getting children
          - serialization
    - how to define hyperparameters and children?
  - what is a node composed of?
    - state
      - name
      - hyperparameters
      - children
    - functionality
      - setting dependencies
      - initializing network state
  - which methods should be instance methods, which should be class methods?
    - architectural logic should be instance methods
      - stateless
      - could be shared between multiple networks
    - network logic should be class methods
      - by updating a shared network state
  - why should the node computations methods be class methods?
    - because it should not depend on the parameters of the node at all
      - hyperaparameters should be read from the network
      - inputs should be specified externally (input_keys)
  - should they be required to be classmethod's / staticmethod's?
    - pros
      - intention is clearer
    - cons
      - more complex interface for extension
    - conclusion
      - no - keep it simple and trust the user not to do something crazy
- where should state be stored?
  - all network state should be stored in a network specific object, not in the nodes
- why not just wrap blocks?
  - pros
    - it seems like there are a lot of parts of it
      - eg. recurrence, attentional, monitoring
  - cons
    - additional complexity
    - it comes with features that are tied togther that we wouldn't want tied together
    - it's recurrence appears to be rather limited
      - the recurrence needs to occur in the apply() method
      - can't use other blocks inside recurrence (?)
- serializing network state
  - do we want to serialize network state, and if so, how do we do it?
    - specifically, not the values of shared variables (which will be serializable for sure), but the values of everything else
    - we probably don't want to serialize theano variables
    - most, if not all, of the state should be recomputable through the architectural tree
    - because treeano is a function from hyperparameters to theano variables, we know that the only state to store is that of shared variables
      - thus
    - there will be other intermediates states (eg. the state of an initialization algorithm (eg. Andrew Saxe initialization)) but those are simply means to an end (where the end is theano variables), which should be already "done" by the time network.build() is done
  - conclusion:
    - serialization of the architecture and shared variable state should be sufficient
- there needs to be separate concepts for function outputs and actual outputs
  - function outputs
    - the output of compute_output
    - function from input variables to output variables representing the computation
    - variables in subsections of the computation graph
  - actual outputs
    - the value of the node with respect to real input in the network
    - variables in the computation graph
  - most of the time they can be the same, but they will be different in the case of output transformations
    - eg. scan / recurrent networks
      - function outputs will be functions for a single time step
      - actual outputs will be sequences over time
  - how to implement it?
    - keep an original_variables map
      - mapping from node_name -> output_name -> variable
      - for all the original outputs of a node
    - keep a current_variables map
      - mapping from node_name -> output_name -> variable
      - for mutated final outputs of a node
      - a scan/recurrent node can edit this map to contain the "actual outputs"
    - network has methods (something like below, not necessarily the exact same name):
      - add_variable
        - adds variable to both original_variables and current_variables
      - replace_variable
        - adds variable to only the final outputs
    - NOTE: names can change in the future
  - rationale for keeping the original variables:
    - these variables can be seen as a function from input to output of a unit
- adding to updates vs adding to cost
  - adding to updates is definitely necessary
    - because some state is just updated
      - eg. rolling mean/variance
  - adding to cost
    - pros
      - if you want to use the same learning algorithm (eg. Adam), it is more convenient
    - cons
      - might be confusing to have 2 ways to do this
    - how?
      - options
        - add to a global cost
        - don't automatically add to costs
          - thus need to manually add to costs
        - have the ability to query for cost of a subtree
        - send the cost into a node that accumulates it, with a default node name
          - eg.
            - Sequential([L2RegularizationNode(), SendTo(node_name="cost")])
            - SumNode(name="cost") # sums all inputs
* assumptions
- shared variables (eg. parameter tensors) will have a fixed size
- nodes are in a directed acyclic graph
  - thus can be traversed in a topological sort
- initialization schemes can work in a topologically sorted order
  - eg. not from output to input
  - not necessarily true, but a simplifying assumption
- shared variables (eg. parameter tensors) are owned by a single node in the computation graph, and that node is responsible for their updates
- node names are strings
- all nodes will have a default output
- shape dimensions of None mean that the shape is unspecified
* random notes
- model everything as a "layer"
  - including loss/objective
  - maybe even represent gradient descent updater as layer node
    - takes in trees of things to update, and loss, and generates updates
  - layer : higher level unit than theano node
    - high level enough that it deserves to both be know and named
  - compose base layers together with functions
    - make sure that functions add a named identity layer
  - use paths instead of strings as names
- issue: how to update an existing architecture to eg. use dropout
  - have a top level assoc_in
    - to add new parameters or replace layers
- responsibilities of each "layer"
  - serialization
  - deserialization
  - output(s)
    - default key for output: "output"
    - map w/ names
    - rationale for multiple outputs:
      - monitoring is one possible use case
  - shape(s)
    - optional: can auto-compute
  - update(s)
    - optional
  - dimension(s)
    - maybe?
- everything has names and paths
  - each node needs a name
  - names are like relative paths
  - paths are absolute in the network
    - tuples of names / indexes
- sharing shared: separate step for initializing shared
  - pass in a map from path to shared, which can be used instead of creating a new shared variable
- sharing weights: separate step for initializing weights
  - pass in a map from path to weights, which can be used instead of creating a new shared variable
- sharing params:
  - use a network.assoc_in(path, value)
    - eg. top_level_network.assoc_in(["deterministic"], True)
- question: when are shareds / weights initialized
  - initializing shared
    - prereq: dimensionality, broadcast dims
  - initializing weights
    - prereq: shape
- state in a node is divide into:
  - hyperparameters
  - shared variables
  - intermediates
- how to do shared initialization:
  - kinds of shared initialization
    - exact path initialization
      - eg. ("FCs", "head", 1, "FC")
    - partial path initialization
      - eg. ("some_initalization", "something", "my_cool_layer", "FC") equivalent to ("my_cool_layer", "FC")
  - question: conceptually, how does one perform initialization with existing shareds
    - SharedInitialization.fromNode(other_network, backup_initialization=NoInitialization())
      - set default to having no init
* features
- modularity
  - kind of a design goal of the other features
  - "tricks" can be made 100% self-contained
- hierarichical specification of hyperparameters
  - customize each subnetwork
- nodes can update themselves
- storing updates as changes in the values instead of new values
  - allow arithmetic on the amount changes
* planned features
- serialization
- easy variable sharing between networks
- easy weight sharing between networks
- auto-"chunk"-ing theano variables
- automatically computing shape of node outputs
- graph transformations
  - eg. give me a new network with all ReLUs substituted for leaky ReLUs
- recurrent networks
* TODOs
** v2
- goals:
  - move everything over to cleaner API
  - have recurrent net working as fast as if done in pure theano w/ strict=True
- order of things:
  - work on RNNs
- have old tests work
- test
  - unit test that before and after building a network, the state of a node is the same
  - test find_hyperparameter
- how should "cost" be treated?
  - options
    - as a specific node
      - pros
        - explitict
        - seems very flexible
      - cons
        - makes it difficult to combine costs together
          - eg. L2 loss
          - would have to each have each "extra" cost specify the main cost, or have the main cost specify each "extra cost"
            - neither option is all that great
    - as a network property / state
      - eg. there is a network state with a key (default="global_loss" or "loss" or "cost") that update nodes by default add to
      - pros
        - makes it easy to add losses together
      - cons
        - new question: when are values added to the cost?
          - during compute_output?
        - uses addition as an implicit function for composition - is this always desired?
    - as the output of a node, then using a SendToNode to send the values as input of a different node (ie. a SummedCostNode)
      - pros
        - allows performing post-processing of values
          - eg.
            - multiplying by a value
            - taking a square
            - making it a margin loss instead
            - composing multiple losses with a max instead of addition
      - cons
        - more verbose
  - conclusion
    - use output of a node, and a SendToNode
- recurrent nets
  - create pseudo-nodes to preprocess output of RNN
    - ie. use sequence variable shape/dtype to create single element input variable of the correct shape
  - nodes
    - RecurrentNode
      - for controlling theano.scan
      - how?
        - find all outputs for node
          - outputs from graph
          - eg. the result of every node is an output
        - find all input sequences for node
          - inputs into graph
        - find all initial states for node
          - eg. RecurrentStateNode
        - find all non_sequences
        - performs scan for all intermediate output values
        - set the variables in the graph to have the appropriate output values (sequences)
    - RecurrentStateNode
      - contains inital state, that may or may not be a learned parameter
      - sets value at time t to be its input at time t
  - computing shape of a scan node
    - for each output variable: set shape to (None, ) + (shape of output variable)
  - theano.scan(strict=True)
    - problem: issue with passing in random variables
      - options
        - ignore and use strict=False
        - have a flag to ensure strict=True
          - somehow verify that the graph has no random variables
    - how to find non_sequences
      - if a node accesses state from somewhere else in the tree (not under the current scan node), treat that output as a non-sequence
      - treat RNG as a non-sequence
      - all shared parameters in the current subtree should be non_sequences (?)
      - maybe have a tag "non_sequences" for variables in the current subtree to be treated as such
        - everything else is treated as an output?
    - how to deal with non_sequences
      - remember which variables are non-sequences and use theano.replace to do a variable replace with the new variables representing the non-sequences
** misc
- cache gradient computation
- sequentialhyperparameternode
- make sure shapes can handle None's for polymorphism
- use assertions for outputs that should not be used
- how to handle updates for RandomStates?
  - options:
    - ignore it and have default_updates handle it
      - it would have to be explicitly ignored by scan
    - add it to the updates
      - problem: what if there are multiple scans - adding the updates don't make sense in this case
      - pros:
        - easier
- grep for FIXME's
  - separate out lasagne initializations from initializations
- try dropout network w/ train / test
  - hyperparameter: have default of enable_dropout be deterministic
- function to wrap lasagne layer boilerplate
  - input:
    - lazy wrapped variable
    - layer constructor
    - kwargs for constructor
  - output: lazy wrapped variable
- test:
  - find_variables
  - preallocatedinitialization
  - theano.tag.test_value
    - http://deeplearning.net/software/theano/tutorial/debug_faq.html
- real initialization nodes
  - using a stateful class and a hyperparameternode is very wrong
    - just a hack
  - want to represent the parameters of the initialization as hyperparameters and have them be clone-able/inspect-able
- PreallocatedInitializationNode
  - sets hyperparameters:
    - shared_initializations
    - preallocated_initialization
  - rationale for setting both:
    - if no initialization scheme is given, then it uses the former
    - if another initialization scheme is given within the network, that scheme still has the choice of finding a preallocated_initialization hyperparameter and prioritizing that
    - this allows no hard-coded preference to preallocation in the core code
- wrap lasagne
  - update code
  - initialization
  - layers
- custom wrapper around variables
  - if shape is not given, print warning and calculate
    - theano.function([x, y], [x], on_unused_input="ignore")(2, 3)
- make insides of update algorithms inspectable (eg. intermediates in nesterov momentum)
** network as data
- create DSL on top of nodes to apply transformations
  - insert_above(), insert_between()
- to_sequentials
  - take in nested lists like hiccup and convert into sequential nodes
    ["a", ["b", node1, node2, ["c", node3]], ["d", ["e", node5]]]
  - maybe have it be very hiccup like:
    ["a", {"num_units": 32},
     ["b",
      ("conv1", "conv", {"num_filters": 42}),
      ("pool1", "pool", {"strides": (3, 3)})],
     ["c",
      ("fc1", "fc"),
      ("fc2", "fc")],
     ("final_fc", "fc", {"num_units": 10}),
     ("output", "softmax")]
- add node metadata
  - eg. "update" "cost" "output" "hyperparameter"
  - this will allow for easier filtering when doing architecture transformations
    - eg. remove update nodes in this subtree
    - can use jQuery like syntax
      - something("node_name", {"type": "update"}).replace(lambda x: ...)
- create graph transformations
  - eg.
    - substitution
      - eg. replace ReLU -> PReLU / leaky ReLU
        - use an update-in operation on the architecture as data that does a tree walk
      - to substitute in only a subtree
    - adding different initialization
** code quality
- PYTHONPATH=~/repos/treeano:$PYTHONPATH sniffer
- potential issue: sharing variables between networks when test_value's are enabled
  - if one network uses the same variables, but with different shapes, the test_value's might break
- testing
** guides / demos / concrete things to build
- temperaturesoftmaxnode
- temperature softmax + categorical cross entropy + rescaling due to temperature (* T^2)
- gradient caching node
  - allows calculating derivatives of parameters with respect to loss high up in the tree (optimization)
- learning rate decay node
  - represents learning rate parameter as theano var w/ updates
- demos
  - vanilla mnist
  - fully connected layer mnist with num_hidden, activation specified once
  - separate conv layers from FC layers
    - different dropout and L2 for conv
- how to do things guide: (thinking in treeano guide)
  - freezing a section
    - use UpdateScaleNode, scale to 0
** future
- use metaclass for registering classes for serialization
- look into drawing:
  - https://github.com/Lasagne/Lasagne/issues/174
    - https://github.com/ebenolson/Lasagne/blob/master/examples/draw_net.py
- javascript interface to expand nodes
  - more expanded = more low-level
- think about:
  - recursive NN
  - RNN
    - beam search
    - bidirectional
    - nested recurrence
      - eg. scan in scan (like a non-fixed size ReNet)
    - attentional models
      - look into attentional interfaces
        - https://github.com/bartvm/blocks/blob/master/blocks/bricks/attention.py
** canopy
- treeano should not have hooks
  - there should be a clean separation of mapping from parameterization to theano variable and all the extra (modular) logic
- canopy
  - layer on top of treeano
  - handles non-theano modularity
  - nice reference
    - the only layer above this is the emergent layer (your code)
- add auto-chunking of variables
  - maybe
- workflows / hooks
  - distillation
    - input: network1, outputkey1, network2, outputkey2, data generator
  - resuming an in-training model
  - optionally resuming an in-training model, or restarting
  - printing progressbar after each epoch
  - checkpoint model every x batches / chunks
  - dynamically changing network during training to things that should be easy in canopy
    - eg. adding additional losses
  - test on validation set every x chunks
  - visualization
    - print filters
    - find most similar inputs to input
      - given:
        - query input
        - possible inputs
        - layer for representation to use
    - given a certain unit, find which inputs fire hardest
    - find inputs that are the most wrong
    - find input that maximally activates for class
      - with constant norm or L1/L2 loss
    - histogram of values of a tensor
      - eg. output of a layer
      - eg. gradient of a weight
    - image visualizations
      - show filter activations for given images
      - show grad(image_class, input)
        - behaves like importance map
      - show input patch that maximally activates a conv neuron
      - deconvolutional networks
  - monitoring
    - printing out certain values
      - eg. loss
      - eg. average magnitude of gradient
    - alerting on certain conditions
      - eg. nan's or inf's
      - eg. magnitude of gradient/weight matrix too high
- blocks callback hooks
  - before_training : bool - is invoked before training.
  - before_first_epoch : bool - is invoked before the first epoch.
  - before_epoch : bool - is invoked before every epoch.
  - on_resumption : bool, optional - is invoked when training is resumed.
  - on_interrupt : bool, optional - is invoked when training is interrupted.
  - after_epoch : bool - is invoked after every epoch.
  - after_batch: bool - is invoked after every batch.
  - after_training : bool - is invoked after training.
  - after_n_epochs : int, optional - if not None, invoked when `after_n_epochs` epochs are done
  - every_n_epochs : int, optional - if not None, invoked after every n-th epoch.
  - after_n_batches : int, optional - if not None, invoked when `after_n_batches` batches are processed.
  - every_n_batches : int, optional - if not None, invoked after every n-th batch.
- datasets
  - RNN
    - Blogger Dataset: http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip (Age and gender data)
      - from https://github.com/IndicoDataSolutions/Passage
    - use MNIST, scan one vector at a time
      - https://github.com/IndicoDataSolutions/Passage/blob/master/examples/mnist.py
- how to save state part-way through training
- seems like a very reasonable assumption that training might involve arbitrary python code execution
  - eg. with deep q learning, might have to simulate a game
- consider hooks
  - implement like clojure handler wrappers?
    - ability to edit givens, updates, etc.
  - would allow alternative workflows
    - eg.
      - curriculum learning
      - adding preprocessing to a model
      - saving plots
      - saving model
  - which hooks?
    - creation of a function
    - training each batch
    - training each chunk
  - question: are hooks local or global
- live plotting/monitoring
  - other tools (blocks / opendeep) use bokeh
