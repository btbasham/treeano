* general
- architecture as data
  - the architecture of a single model should at every point be serializable and deserializable to the same model
- parameterization should be recursive
  - eg inner parameterization inherit outer parameterization unless explicitly overwritten
    - maybe: parameterization such as update strategy and initialization
  - use this for things like dropout
- shape must be knowable statically, at least for a fraction of the network
  - before learned weight initialization
    - eg. so that fully connected layers know their size
  - not all layers require shape
    - eg. conv/maxpool/spatial pyramid pooling
    - eg. if there is a spatial pyramid pooling layer, don't need to know shapes of conv layers
- all nodes should behave like data and only data
  - the interface should be what data it contains
  - possibly lazy data, though
- serialization
  - model architecture should be serialized in human-readable / edittable format
    - eg. yaml
    - being able to be serialized to json is probably a pretty safe format
      - though not necessarily serialized to json
  - model weights should be in some agnostic key-value store format
    - a similar architecture should be able to read in weights perfectly
    - path -> np.array
  - should be possible to share weights via specifying a similar architecture
    - since the layer names are absolutely critical to what "similar" means, it seems safe to assume that this sharing can come of the form of converting the other net into this key->value form and having the new net simply read from this blob
  - all layers can specify how they should be serialized (as data)
- input parameters
  - assumption: local networks (local = created together / close together) will have similar parameters
  - thus we can have local "inheritance" of parameters
    - eg. outer layer has an immutable dict of parameters, inner layer overwrites them when it has a parameter override, more inner layers can read from parameters with a key
    - this requires non-ambiguous keys
      - ie. 2 layers shouldn't have the same key mean different things
- need metadata on weight parameters
  - still need to specify backprop-able params, since the gradient needs to be calculated
  - still need to specify non-bias params, for regularization?
    - not necessarily, if the layer knows how to handle it's own parameters
    - this makes it easier if there is a large number of layers expected to be handle the same way
    - "the expression problem"
      - you might add a new layer that would want to benefit from all the regularization
      - you might add new regularization that would wnat to benefit from all layers
      - what commonality can we extract from this?
        - different parameters can behave the same way
        - eg. bias / inputweights
          - makes sense to have an "escape hatch" where things can work if not fitting within a mold
  - parameters should have tagging
    - to work with multiple kinds of regularization
- question: should all nodes have a unique name?
  - pros of unique name
    - don't need separate concepts for unique / non-unique
      - and some names do need to be unique (for initialization between networks)
  - cons
    - conceptually unnecessary, as long as they have a unique path
      - eg. don't need to name "convpoolstack_pool" when "pool" has parent "convpoolstack"
        - this would also be DRY-er and make the architectures more human readable
  - conclusion: just go with everything having a unique name
    - this should be easier to understand - and the downsides aren't that bad
- question: should hyperparameters have global default values
  - eg. shared initialization (create the var)
  - eg. weight initialization (0s)
  - answer: see "question: should parameters and shared variables have different initializations?"
- question: should parameters and shared variables have different initializations?
  - pros of different:
    - they are 2 very different things, no reason to have one know about the other
      - eg. Uniform initialization shouldn't have to know how to create a shared variable
    - if they were the same, and there was a subnetwork with different initialization, that subnetwork couldn't be initialized with shared variables from another network
      - this means that there must be a mechanism by which the shared initialization can overwrite that initialization
  - cons of different:
    - this is a purely theano-level concern, thus a conceptual network shouldn't require it
    - when using shared variables from a different network, you probably don't want to re-initialize
  - conclusion:
    - should be different
      - and a weight initialization node should look up the tree for a hyperparameter on shared initialization
    - weight initialization should have a default shared initialization
      - thus no global shared initialization should be needed
    - weight initialization and shared variable creation should happen at the same time, so that weight initialization knows now to overwrite the values in a shared variable
- should lifecycle methods (init_state, init_shared, init_values, and compute) be separate?
  - pros of separation
    - easier to do special schemes like loading existing network
    - may allow input layers to be partially initialized before initializing
  - cons
    - more boilerplate
    - somewhat more elegant to define everything in a single function
      - might not even need laziness
  - misc
    - seems unlikely that there would be a good use for having partially initialized layers
  - thought: init_state is different from the others - since initializing the variables can be lazy
    - alternative: pre_compute and post_compute steps for pre-order and post-order traversal
    - pros of separating only init_state
      - if they happen all at once, some nodes would need a pre-order traversal (eg. an initialization node that creates a stateful initializer), while pretty much all nodes need a post-order traversal (to specify output)
  - conclusion:
    - separate init_state, but have a single compute step which:
      - creates shared variables (parameters)
      - creates outputs
      - sets initial values of shared variables (parameters)
- using networkx to represent the graph
  - pros
    - a bunch of stuff for free
      - make it easy to look for ancestors, etc.
      - topological sort
      - make cute diagram of an architecture
  - cons
    - extra dependency
- should architecture nodes be separate from state nodes?
  - they conceptually serve 2 very different purposes
  - but there implementations are linked together
    - ie. the data in an architecture node is used for a state node
  - separating them would require additional verbosity to combine them together
  - conclusion: keep them together, but make sure not to mutate them
- question: where are dependencies defined?
  - NOTE: only containers need to define dependencies
  - question: how to define a node which is shared between multiple trees (eg. the same thing is processed in 2 places)
    - just because something is build like a tree, doesn't mean there can't be jumps in it
      - can have a "get_layer_with_name" node
  - conclusion:
    - parents automatically depend on their children
    - a "get_layer_with_name" node depends on that layer
    - more dependencies can be defined within nodes
- question: should we be able to add nodes in the graph after build-ing?
  - eg. add special intermediate nodes
  - use case: container node wants to create some sort of state that it's children can depend on
  - conclusion: no
    - one would have to make sure it's initialized just like in build
    - this kind of change seems like it would make it easy to violate assumptions / principle of least surprise
- how to do composite node dependencies properly
  - ie. how to pass input of sequential node to first node
  - how do you have it return the output of its last child? (sequential node)
    - the input of a composite node will be set by its parent - thus it can't set it's own dependencies on its children
  - how to express dependencies of first child:
    - first child depends on input of sequential node
    - last child depends on other children
    - sequential node output depends on last child
    - if first child depends on sequential node, this causes a cycle
  - there is a true dependency between the input of the sequential node and its first child
    - thus it makes sense to make this explicit
  - conclusion:
    - walk tree from top-level to bottom level, so that a dependency between the input of the sequential node and the first child can be set
  - alternative:
    - some form of graph unification
      - eg.
        - unify inputs of sequential node with inputs of first child
        - unify outputs of sequential node with outputs for last child
- question: how are inputs passed into a node?
  - parent node should handle it
- question: how should the input to a node be passed in?
  - options
    - function argument
      - pros
        - more expected
        - can enfore the right arity
      - cons
        - can't specify which key in the output map
    - through the graph
      - eg. self.graph.my_input(some_key="output")
    - through the graph w/ sugar
      - eg. self.get_input(some_key="output")
  - conclusion:
    - through the graph w/ sugar
    - no positional arguments (much harder to specify in a graph)
- why have wrapped variables?
  - additonal metadata
    - eg. shape, tags
  - custom rules
    - eg. related to tags
      - shared variables should be parameters and either weight / bias
      - weight / bias -> also parameter
    - eg. parameters must be shared
  - why make them lazy?
    - original motivation: to allow stage-wise initialization
      - eg.
        - first create the shareds
        - then perform operation with them
        - then adds some values in
  - pros of unwrapped variables
    - can directly look at the computation graph for dependencies
  - solution:
    - include a reference to the wrapped variable in the theano variable
      - so that the computation graph can still be walked for dependencies
- why not using tagging on theano variables?
  - because theano variables can be shared between networks
    - the state of one network shouldn't be messed with the state of another
- question: should nodes update themselves or should a higher level node do it?
  - pros if nodes updates themselves:
    - nodes can make sure each update only occurs once
  - pros of higher level node doing it:
    - code might be cleaner - because it can update all the weights in a single batch
  - question: is there any way to set higher level nodes to only update the weight of a unit once?
    - or can a lower level node override those updates
  - conclusion: higher level node can do it, lower level nodes can override
- how to compute update deltas?
  - compute bottom-up in architectural tree
    - pros:
      - can be done by adding immutable UpdateDeltas together
  - compute top-down in architectural tree
    - so children can overwrite / add
    - pros:
      - more complex logic can happen in a single mode
    - cons:
      - if using a single mutable UpdateDeltas, then inner nodes can see state of nodes that aren't its parents
        - eg.
          - b<-x->a (x w/ children a,b)
          - x computes its updates
          - a can see x's updates
          - thus a can also see updates for node b from x
          - whichever of a,b is computed second will see updates from the other
        - this may cause errors as well if one accidentaly updates weights that they shouldn't be able to see
  - add it all together
    - must be equivalent to an implicit top-down
      - eg. in order for a node to zero out updates to its children, some sort of top-down process must occur
    - pros
      - simpler interface
    - cons
      - complexity must be implemented anyway - why not expose it
  - conclusion: top-down
    - rationale:
      - lower in the tree is more specific, and you'd more likely want to perform an effect on a general setting than apply a more general effect to independent local settings
        - eg. if you want to zero out updates, more likely a non-local update than lots of independent local updates
- how should updatedeltas be updated (assuming top-down / root-to-leaf computation)?
  - it seems that either implicitly or explicitly, update deltas need to be passed into nodes
    - how else would a node zero out the updates?
      - doing so declaratively is a (complex) possibility
        - especially a simplified version with only the ability to add / overwrite
  - options
    - update_deltas should be passed in to a compute_update_deltas function
      - pros
        - consistent with normal python functions
    - update_deltas should be accessed as a attribute of the current node
      - pros
        - makes it clear where they are stored, always being in the same place
        - they should be accessed as an attribute in the future
  - conclusion:
    - update_deltas should be shared for a network
      - because
        - a single network should have a single set of updates
        - every node gets access to the updates anyway via compute_update_deltas
        - the updates should be stored somewhere
          - since they don't change, they can be cached
        - instead of storing then in some arbitrary node, we can store them in all nodes
    - update_deltas should be passed in to a compute_update_deltas function
      - rationale: simpler, makes sense, and might be useful for re-applying/re-ordering updates
      - note: should assert that compute_update_deltas returns None to make the interface clearly be one for mutation
    - because update deltas will be shared and mutable, their api should be made such that it isn't ease to accidentally mess than up
      - eg. use __iadd__ instead of __add__
- should computing output and update deltas happen at the same time?
  - no, because some node's updates will require outputs computed later in the DAG
    - eg. recurrent state would naturally have a loop
- the input arguments for computing the output of a node should be a function of the network
  - for nodes that dynamically get new input keys
    - eg. a node that is sent input
      - eg. a cost node that all costs are sent to
- node API
  - architecture nodes ONLY have 2 things: hyperparameters and children
    - in various desired formats
      - might want custom ways of inputting children
        - eg. networkx DAG
      - might want custom ways of inputting hyperparameters
        - eg. hyperparameter node
          - takes in kwargs
        - eg. something that provides stateful hyperparameters
    - how to effectively define these things in an easy to use way?
      - to minimize overall work + maximize simplicity
        - specifically in:
          - __init__
          - getting hyperparameters
          - getting children
          - serialization
    - how to define hyperparameters and children?
  - what is a node composed of?
    - state
      - name
      - hyperparameters
      - children
    - functionality
      - setting dependencies
      - initializing network state
  - which methods should be instance methods, which should be class methods?
    - architectural logic should be instance methods
      - stateless
      - could be shared between multiple networks
    - network logic should be class methods
      - by updating a shared network state
  - why should the node computations methods be class methods?
    - because it should not depend on the parameters of the node at all
      - hyperaparameters should be read from the network
      - inputs should be specified externally (input_keys)
  - should they be required to be classmethod's / staticmethod's?
    - pros
      - intention is clearer
    - cons
      - more complex interface for extension
    - conclusion
      - no - keep it simple and trust the user not to do something crazy
- where should state be stored?
  - all network state should be stored in a network specific object, not in the nodes
- why not just wrap blocks?
  - pros
    - it seems like there are a lot of parts of it
      - eg. recurrence, attentional, monitoring
  - cons
    - additional complexity
    - it comes with features that are tied togther that we wouldn't want tied together
    - it's recurrence appears to be rather limited
      - the recurrence needs to occur in the apply() method
      - can't use other blocks inside recurrence (?)
- serializing network state
  - do we want to serialize network state, and if so, how do we do it?
    - specifically, not the values of shared variables (which will be serializable for sure), but the values of everything else
    - we probably don't want to serialize theano variables
    - most, if not all, of the state should be recomputable through the architectural tree
    - because treeano is a function from hyperparameters to theano variables, we know that the only state to store is that of shared variables
      - thus
    - there will be other intermediates states (eg. the state of an initialization algorithm (eg. Andrew Saxe initialization)) but those are simply means to an end (where the end is theano variables), which should be already "done" by the time network.build() is done
  - conclusion:
    - serialization of the architecture and shared variable state should be sufficient
- there needs to be separate concepts for function outputs and actual outputs
  - function outputs
    - the output of compute_output
    - function from input variables to output variables representing the computation
    - variables in subsections of the computation graph
  - actual outputs
    - the value of the node with respect to real input in the network
    - variables in the computation graph
  - most of the time they can be the same, but they will be different in the case of output transformations
    - eg. scan / recurrent networks
      - function outputs will be functions for a single time step
      - actual outputs will be sequences over time
  - how to implement it?
    - keep an original_variables map
      - mapping from node_name -> output_name -> variable
      - for all the original outputs of a node
    - keep a current_variables map
      - mapping from node_name -> output_name -> variable
      - for mutated final outputs of a node
      - a scan/recurrent node can edit this map to contain the "actual outputs"
    - network has methods (something like below, not necessarily the exact same name):
      - add_variable
        - adds variable to both original_variables and current_variables
      - replace_variable
        - adds variable to only the final outputs
    - NOTE: names can change in the future
  - rationale for keeping the original variables:
    - these variables can be seen as a function from input to output of a unit
- adding to updates vs adding to cost
  - adding to updates is definitely necessary
    - because some state is just updated
      - eg. rolling mean/variance
  - adding to cost
    - pros
      - if you want to use the same learning algorithm (eg. Adam), it is more convenient
    - cons
      - might be confusing to have 2 ways to do this
    - how?
      - options
        - add to a global cost
        - don't automatically add to costs
          - thus need to manually add to costs
        - have the ability to query for cost of a subtree
        - send the cost into a node that accumulates it, with a default node name
          - eg.
            - Sequential([L2RegularizationNode(), SendTo(node_name="cost")])
            - SumNode(name="cost") # sums all inputs
- how should "cost" be treated?
  - options
    - as a specific node
      - pros
        - explitict
        - seems very flexible
      - cons
        - makes it difficult to combine costs together
          - eg. L2 loss
          - would have to each have each "extra" cost specify the main cost, or have the main cost specify each "extra cost"
            - neither option is all that great
    - as a network property / state
      - eg. there is a network state with a key (default="global_loss" or "loss" or "cost") that update nodes by default add to
      - pros
        - makes it easy to add losses together
      - cons
        - new question: when are values added to the cost?
          - during compute_output?
        - uses addition as an implicit function for composition - is this always desired?
    - as the output of a node, then using a SendToNode to send the values as input of a different node (ie. a SummedCostNode)
      - pros
        - allows performing post-processing of values
          - eg.
            - multiplying by a value
            - taking a square
            - making it a margin loss instead
            - composing multiple losses with a max instead of addition
      - cons
        - more verbose
  - conclusion
    - use output of a node, and a SendToNode
- question: how to create nodes from other nodes?
  - why this is a problem?
    - it exposes implementation details
    - creates complexity at deserialization time
      - when constructed, generated nodes are not present
      - when deserialized, they are
  - options
    - psuedo-nodes
      - nodes that are in the DAG, but not serialized
      - pros
        - very composable, works exactly the same as a normal node
      - cons
        - adds noise to visualizing the DAG
          - this might not be that big of a deal
        - seems a little inelegant
      - how to implement?
        - have a serialization_children() function
          - pros
            - makes it known that it is a special case
          - cons
            - need to add to children, then later remove from them instead of handling logic of pseudo children in one place
            - works only 1 level deep
              - ie. can't make a child of a child a pseudo-node
        - overwrite architectural_children() to add the nodes
          - and have NodeImpl use ._children instead of .children
          - pros
            - more elegant that serialization_children() function
          - cons
            - still only works 1 level deep
        - create PseudoNode wrapper around node which gets ignored by serialization
          - pros
            - explicit, since it provides additional metadata
            - most composable
              - solves the "1 level deep" problem
          - how?
            - create BaseNode class that NodeAPI inherits from
            - make PseudoNode inherit from BaseNode
            - overwrite getattr to just behave like the wrapped node
            - implement ignoring in graph
            - to instantiate, intercept __init__ call
    - save the original children, transform the children lazily
      - ie. have the new children added only on architecture_children()
      - you want the original input, thus the original input MUST be stored somewhere
        - is it a hyperparameter, children, or is the API insufficient?
          - it behaves nothing like other hyperparameters
            - because it's effect occurs before the graph is constructed
          - it behaves very similarly to children
      - question: should transformed input be cached?
        - pros
          - can be faster
          - should be fine, because the architecture_children should be a pure function of the node's parameters
        - cons
          - mutates the node
- ScanNode
  - for controlling theano.scan
  - how?
    - find all input sequences for node
      - inputs into graph
      - needs to go to ScanInputNode
    - find all outputs for node
      - outputs from graph
        - find_variables_in_subtree(["output"])
      - eg. the result of every node is an output
    - find all initial states for node
      - eg. ScanStateNode
    - find all non_sequences
    - performs scan for all intermediate output values
    - set the variables in the graph to have the appropriate output values (sequences)
- ScanStateNode
  - contains inital state, that may or may not be a learned parameter
  - sets value at time t to be its input at time t
  - 3 conceptual kinds
    - constant initial state
    - shared initial state
      - potentially learned
    - initial state from the network
      - NOTE: could consider the 2 other cases as special cases of this one, where the constant state or shared state is a special node
  - hyperparameters
    - where in the network to initialize state from
      - this is a hyperparameter because taking the initial state as the input to the node is almost certainly wrong in the context of a scan node (you want to initialize with a value that is not dependent on time)
    - how many time steps back to look
- what is the proper way of initializing long-range dependencies?
  - these behave differently from other dependencies and cannot occur in the normal order
    - eg. if a SendToNode sends its output to a SequentialNode that has already had its state/dependencies initialized, this messes up the dependencies of the latter's children
  - what order makes sense for init_state?
    - going top down makes sense
      - rationale: if a parent sets up dependencies for its children, children can then manipulate/introspect their dependencies
        - eg. SequentialNode currently uses this
  - options
    - special case SendToNode's in network.build to call it before all others
      - cons
        - inelegant
    - have an init_long_range_dependencies method for nodes
      - has no guarantees about which order these would be called in
      - pros
        - more general than special casing SendToNode
      - cons
        - adds more to the node interface
    - make init_state idempotent and calling it again
      - pros
        - it's generally nicer to have things idempotent
        - can ensure that init_state is called in an order that makes sense
      - cons
        - (unnecessarily?) limiting
        - some amount of upfront work
        - might be unsufficient
          - ie. might have to undo some effects - which will lead to it being limiting
    - make init_state not dependent on network topology
      - pros
        - simpler (?)
      - cons
        - very limiting
          - how to solve the sequential node input forwarding issue?
  - conclusion
    - going with init_long_range_dependencies
      - most general (ie. will allow other nodes to reimplement what SendToNode does) while solving the problem
* assumptions
- shared variables (eg. parameter tensors) will have a fixed size
- nodes are in a directed acyclic graph
  - thus can be traversed in a topological sort
- initialization schemes can work in a topologically sorted order
  - eg. not from output to input
  - not necessarily true, but a simplifying assumption
- shared variables (eg. parameter tensors) are owned by a single node in the computation graph, and that node is responsible for their updates
- node names are strings
- all nodes will have a default output
- shape dimensions of None mean that the shape is unspecified
* random notes
- model everything as a "layer"
  - including loss/objective
  - maybe even represent gradient descent updater as layer node
    - takes in trees of things to update, and loss, and generates updates
  - layer : higher level unit than theano node
    - high level enough that it deserves to both be know and named
  - compose base layers together with functions
    - make sure that functions add a named identity layer
  - use paths instead of strings as names
- issue: how to update an existing architecture to eg. use dropout
  - have a top level assoc_in
    - to add new parameters or replace layers
- responsibilities of each "layer"
  - serialization
  - deserialization
  - output(s)
    - default key for output: "output"
    - map w/ names
    - rationale for multiple outputs:
      - monitoring is one possible use case
  - shape(s)
    - optional: can auto-compute
  - update(s)
    - optional
  - dimension(s)
    - maybe?
- everything has names and paths
  - each node needs a name
  - names are like relative paths
  - paths are absolute in the network
    - tuples of names / indexes
- sharing shared: separate step for initializing shared
  - pass in a map from path to shared, which can be used instead of creating a new shared variable
- sharing weights: separate step for initializing weights
  - pass in a map from path to weights, which can be used instead of creating a new shared variable
- sharing params:
  - use a network.assoc_in(path, value)
    - eg. top_level_network.assoc_in(["deterministic"], True)
- question: when are shareds / weights initialized
  - initializing shared
    - prereq: dimensionality, broadcast dims
  - initializing weights
    - prereq: shape
- state in a node is divide into:
  - hyperparameters
  - shared variables
  - intermediates
- how to do shared initialization:
  - kinds of shared initialization
    - exact path initialization
      - eg. ("FCs", "head", 1, "FC")
    - partial path initialization
      - eg. ("some_initalization", "something", "my_cool_layer", "FC") equivalent to ("my_cool_layer", "FC")
  - question: conceptually, how does one perform initialization with existing shareds
    - SharedInitialization.fromNode(other_network, backup_initialization=NoInitialization())
      - set default to having no init
* features
- modularity
  - kind of a design goal of the other features
  - "tricks" can be made 100% self-contained
- hierarichical specification of hyperparameters
  - customize each subnetwork
- nodes can update themselves
- storing updates as changes in the values instead of new values
  - allow arithmetic on the amount changes
* planned features
- serialization
- easy variable sharing between networks
- easy weight sharing between networks
- auto-"chunk"-ing theano variables
- automatically computing shape of node outputs
- graph transformations
  - eg. give me a new network with all ReLUs substituted for leaky ReLUs
- recurrent networks
* TODOs
** v2
- goals:
  - have recurrent net working as fast as if done in pure theano w/ strict=True
- for cost:
  - SumNode
- CombineNode with a combine function
- recurrent.py
- DenseCombineNode
  - SplitCombine + LinearNode each + combine with sum + BiasNode
- FunctionCombineNode
- LinearMappingNode
  - can copy weights w/ tag copy into parent node, for debuggability
- allow theano var for inputnode to be a parameter to be passed in
- have hyperparameter precedence favor closer to the node over more specific hyperparameter name
  - eg. if conv layer defines strides, but hyperparamnode defines conv_strides, use conv's hyperparameter
- test
  - add serialization tests for all nodes
- more sophisticated way of finding nodes in subtree
  - eg. finding scan state nodes whose scan is the current scan
*** recurrent test
#+BEGIN_SRC python
  import numpy as np
  import theano
  import theano.tensor as T


  def binary_toy_data(lag=1, length=20):
      inputs = np.random.randint(0, 2, length)
      outputs = np.array(lag * [0] + list(inputs))[:length]
      return inputs, outputs


  floatX = theano.config.floatX
  hidden_state_size = 10

  W_x = theano.shared(
      (0.1 * np.random.randn(1, hidden_state_size)).astype(floatX))
  W_h = theano.shared(
      (0.1 * np.random.randn(hidden_state_size,
                             hidden_state_size)).astype(floatX))
  W_y = theano.shared(
      (0.1 * np.random.randn(hidden_state_size, 1)).astype(floatX))


  X = T.tensor3("X")
  Y = T.tensor3("Y")


  def step(x, h):
      new_h = T.tanh(T.dot(x, W_x) + T.dot(h, W_h))
      new_y = T.nnet.sigmoid(T.dot(new_h, W_y))
      return new_h, new_y


  results, updates = theano.scan(
      fn=step,
      sequences=[X],
      outputs_info=[T.patternbroadcast(T.zeros((1, 10)), (False, False)), None],
  )
  ys = results[1]

  loss = T.mean(T.nnet.categorical_crossentropy(ys, Y))
  loss = T.mean((ys - Y) ** 2)
  import lasagne
  updates = lasagne.updates.sgd(loss, [W_x, W_h, W_y], 0.1)

  train_fn = theano.function([X, Y], loss, updates=updates)
  valid_fn = theano.function([X], ys)

  LAG = 20
  LENGTH = 50

  for _ in range(5000):
      inputs, outputs = binary_toy_data(lag=LAG, length=LENGTH)
      loss = train_fn(inputs.reshape(-1, 1, 1), outputs.reshape(-1, 1, 1))
      print(loss)

  inputs, outputs = binary_toy_data(lag=LAG, length=LENGTH)
  preds = valid_fn(inputs.reshape(-1, 1, 1)).flatten()
#+END_SRC
*** WIP scan recurrent
#+BEGIN_SRC python
  import time
  import numpy as np
  import theano
  import theano.tensor as T
  import lasagne
  import pylab
  import seaborn
  import dslib
  from d.sandbox.nn_utils import adam_v4

  import treeano
  from treeano.lasagne.nodes import DenseNode, ReLUNode, SGDNode


  def fX(arr):
      return arr.astype(theano.config.floatX)


  def binary_toy_data(lag=1, length=20):
      inputs = np.random.randint(0, 2, length)
      outputs = np.array(lag * [0] + list(inputs))[:length]
      return fX(inputs), fX(outputs)


  LAG = 10
  LENGTH = 50
  hidden_state_size = 40


  treeano.lasagne.nodes.SGDNode(
      "sgd",
      learning_rate=0.01,
      children=treeano.nodes.ContainerNode(
          "c1",
          [
              treeano.nodes.InputNode(
                  "y",
                  shape=(LENGTH, 1, 1)),
              treeano.nodes.SequentialNode(
                  "s1",
                  [treeano.nodes.InputNode(
                      "x",
                      shape=(LENGTH, 1, 1)),
                   treeano.nodes.scan.ScanNode(
                       "scan",
                       treeano.nodes.SequentialNode(
                           "s2",
                           [treeano.nodes.IdentityNode(
                               "x-elem"),
                            treeano.nodes.scan.ScanStateNode(
                                "h",
                                initial_state=TODO,
                                next_state=TODO),
                            treeano.nodes.lasagne.DenseNode(
                                "y_pre",
                                num_units=1),
                            treeano.nodes.SigmoidNode(
                                "pred")]),
                       scan_axis=0),
                   treeano.nodes.CostNode(
                       "cost",
                       reference="target")
                  ]),
          ]))

  W_x = theano.shared(
      fX(0.1 * np.random.randn(1, hidden_state_size)))
  W_h = theano.shared(
      fX(0.1 * np.random.randn(hidden_state_size, hidden_state_size)))
  W_y = theano.shared(
      fX(0.1 * np.random.randn(hidden_state_size, 1)))


  X = T.tensor3("X")
  Y = T.tensor3("Y")


  def step(x, h):
      new_h = nonlin(T.dot(x, W_x) + T.dot(h, W_h))
      new_y = T.nnet.sigmoid(T.dot(new_h, W_y))
      return new_h, new_y


  results, updates = theano.scan(
      fn=step,
      sequences=[X],
      outputs_info=[T.patternbroadcast(T.zeros((1, hidden_state_size)),
                                       (False, False)),
                    None],
  )
  ys = results[1]

  LAG = 10
  LENGTH = 50


  loss = T.mean(T.nnet.categorical_crossentropy(ys, Y))
  loss = T.mean((ys[LAG:] - Y[LAG:]) ** 2)
  loss = T.mean((ys - Y) ** 2)
  # updates = lasagne.updates.sgd(loss, [W_x, W_h, W_y], 0.1)
  updates = adam_v4(loss, [W_x, W_h, W_y])

  train_fn = theano.function([X, Y], loss, updates=updates)
  valid_fn = theano.function([X], ys)

  start_time = time.time()
  losses = []
  for _ in range(5000):
      inputs, outputs = binary_toy_data(lag=LAG, length=LENGTH)
      loss = train_fn(inputs.reshape(-1, 1, 1), outputs.reshape(-1, 1, 1))
      losses.append(loss)

  pylab.plot(losses, 'o')
  pylab.show()
  total_time = time.time() - start_time
  inputs, outputs = binary_toy_data(lag=LAG, length=LENGTH)
  preds = valid_fn(inputs.reshape(-1, 1, 1)).flatten()

  print((total_time, np.mean(losses[-100:])))
#+END_SRC
** misc
- add check in compute_output phase to make sure network doesn't return additional inputs
  - rationale: they do not get recorded in the node's "inputs"
    - thus makes it harder to compute shape
- add shortcuts for nodes that wrap other nodes
  - eg. to inherit their hyperparameters
- think about how to specify if a node owns a weight
  - perhaps if nodes that used parameters had ParameterNode's, it would be easy to have a drop in replacement as a reference to a parameter elsewhere in the network
- composite nodes
  - sequentialhyperparameternode
  - DenseNode = As2DNode + LinearMappingNode + BiasNode
  - LinearMappingNode = ParameterNode + DotNode
- relativenetwork
  - add __repr__
  - add .W .b syntax shortcut for accessing state
  - add get_state set_state which does not overlap with current_variables
- add __repr__ for variablewrapper
- make sure all intermediate variables are named
  - for ease of debugging
- implement
  - LSTM / GRU
    - https://github.com/fchollet/keras/blob/master/keras/layers/recurrent.py
    - https://github.com/mila-udem/blocks/blob/master/blocks/bricks/recurrent.py
- cache gradient computation
- make sure shapes can handle None's for polymorphism
- use assertions for outputs that should not be used
- grep for FIXME's
  - separate out lasagne initializations from initializations
- try dropout network w/ train / test
  - hyperparameter: have default of enable_dropout be deterministic
- function to wrap lasagne layer boilerplate
  - input:
    - lazy wrapped variable
    - layer constructor
    - kwargs for constructor
  - output: lazy wrapped variable
- test:
  - find_variables
  - preallocatedinitialization
  - theano.tag.test_value
    - http://deeplearning.net/software/theano/tutorial/debug_faq.html
- real initialization nodes
  - using a stateful class and a hyperparameternode is very wrong
    - just a hack
  - want to represent the parameters of the initialization as hyperparameters and have them be clone-able/inspect-able
- PreallocatedInitializationNode
  - sets hyperparameters:
    - shared_initializations
    - preallocated_initialization
  - rationale for setting both:
    - if no initialization scheme is given, then it uses the former
    - if another initialization scheme is given within the network, that scheme still has the choice of finding a preallocated_initialization hyperparameter and prioritizing that
    - this allows no hard-coded preference to preallocation in the core code
- wrap lasagne
  - update code
  - initialization
  - layers
- custom wrapper around variables
  - if shape is not given, print warning and calculate
    - theano.function([x, y], [x], on_unused_input="ignore")(2, 3)
- make insides of update algorithms inspectable (eg. intermediates in nesterov momentum)
- tree of hyperparameter precedence
  - instead of having each node define the fallbacks of each hyperparameter, have it reference a tree so that shared hyperparameters have the same fallbacks
  - pros
    - consistency for users
  - cons
    - might be complex to understand
    - defining hyperparameters in more than one place
** guides / demos / concrete things to build
- try building:
  - triplet network
  - network with parameter sharing
    - eg. autoencoder with transpose as weight
- temperaturesoftmaxnode
- temperature softmax + categorical cross entropy + rescaling due to temperature (* T^2)
- gradient caching node
  - allows calculating derivatives of parameters with respect to loss high up in the tree (optimization)
- learning rate decay node
  - represents learning rate parameter as theano var w/ updates
- demos
  - vanilla mnist
  - fully connected layer mnist with num_hidden, activation specified once
  - separate conv layers from FC layers
    - different dropout and L2 for conv
- how to do things guide: (thinking in treeano guide)
  - freezing a section
    - use UpdateScaleNode, scale to 0
** network as data
- create DSL on top of nodes to apply transformations
  - insert_above(), insert_between()
- to_sequentials
  - take in nested lists like hiccup and convert into sequential nodes
    ["a", ["b", node1, node2, ["c", node3]], ["d", ["e", node5]]]
  - maybe have it be very hiccup like:
    ["a", {"num_units": 32},
     ["b",
      ("conv1", "conv", {"num_filters": 42}),
      ("pool1", "pool", {"strides": (3, 3)})],
     ["c",
      ("fc1", "fc"),
      ("fc2", "fc")],
     ("final_fc", "fc", {"num_units": 10}),
     ("output", "softmax")]
- add node metadata
  - eg. "update" "cost" "output" "hyperparameter"
  - this will allow for easier filtering when doing architecture transformations
    - eg. remove update nodes in this subtree
    - can use jQuery like syntax
      - something("node_name", {"type": "update"}).replace(lambda x: ...)
- create TransformationNode
  - applies transformations lazily
    - ie. not immediately but only when the network asks for its children
  - pros
    - lazy
      - work is done as late as possible
    - declarative
      - you show how to get the complex result, not the complex result on its own
      - readable
  - ie. AddDropoutBeforeDenseNode
- create graph transformations
  - eg.
    - substitution
      - eg. replace ReLU -> PReLU / leaky ReLU
        - use an update-in operation on the architecture as data that does a tree walk
      - to substitute in only a subtree
    - adding different initialization
** code quality
- PYTHONPATH=~/repos/treeano:$PYTHONPATH sniffer
- potential issue: sharing variables between networks when test_value's are enabled
  - if one network uses the same variables, but with different shapes, the test_value's might break
- testing
** RNNs
- add truncate_gradient parameter
- taps for recurrent nets
- how to handle updates for RandomStates?
  - options:
    - ignore it and have default_updates handle it
      - it would have to be explicitly ignored by scan
    - add it to the updates
      - problem: what if there are multiple scans - adding the updates don't make sense in this case
      - pros:
        - easier
- theano.scan(strict=True)
  - problem: issue with passing in random variables
    - options
      - ignore and use strict=False
      - have a flag to ensure strict=True
        - somehow verify that the graph has no random variables
  - how to find non_sequences
    - if a node accesses state from somewhere else in the tree (not under the current scan node), treat that output as a non-sequence
    - treat RNG as a non-sequence
    - all shared parameters in the current subtree should be non_sequences (?)
    - maybe have a tag "non_sequences" for variables in the current subtree to be treated as such
      - everything else is treated as an output?
  - how to deal with non_sequences
    - remember which variables are non-sequences and use theano.replace to do a variable replace with the new variables representing the non-sequences
- how to deal with multiple sequences to recur over?
  - eg. sensor setting
    - sequences: some visual signal, some numeric signal
  - alternatives
    - have scaninputnode take in all inputs of scannode and forward them along
      - then need reference node to read in the proper output variable from the scan input node
      - pros
        - semantically correct
      - cons
        - need the scaninputnode's name
          - solution: name can be looked up like a hyperparameter
    - have RecurrentInputNode's be manually specified, along with the key to be passed in
      - looks at the input of the scan node with the key, and creates an input variable of the proper shape
      - pros
        - simpler than previous
      - cons
        - the "normal" case of a single input is harder
        - creates more "noise" in the graph
    - have a ScanAwareReferenceNode
      - the node is aware of scan's semantics and performs the appropriate transformation of the variable
      - this reference node can reference variables outside the scan, and the input gets automatically converted to an element from a sequence
      - pros
        - simplest of all
        - most similar to non-scan functionality (referencenode)
        - can add afterwards, thus allowing implementing the default case first
      - cons
        - doesn't work for nested scans (how do you know how much un-sequencing look like)
          - solution: make it explicit about how many scan levels up/down to convert a variable
    - make sequences / forwarding be a parameter of ScanNode (takes in a map of nodes and which outputs to forward to)
  - conclusion:
    - for now, use the ScanAwareReferenceNode seems like the best solution
    - because it is compatible with the single output design, we can do this later
** future
- make python3 compatible
- use metaclass for registering classes for serialization
- look into drawing:
  - https://github.com/Lasagne/Lasagne/issues/174
    - https://github.com/ebenolson/Lasagne/blob/master/examples/draw_net.py
- javascript interface to expand nodes
  - more expanded = more low-level
- parallelism for running gpu networks on a different process
  - https://gist.github.com/albertz/4177e40d41cb7f9f7c68
- think about:
  - recursive NN
  - RNN
    - beam search
    - bidirectional
    - nested recurrence
      - eg. scan in scan (like a non-fixed size ReNet)
    - attentional models
      - look into attentional interfaces
        - https://github.com/bartvm/blocks/blob/master/blocks/bricks/attention.py
** canopy
- for each hook, document:
  - when it occurs
    - eg. on compile, after each batch, after each chunk, etc.
- problem: how to have decomplected monitoring
  - want to monitor completely different weights in the same way
- treeano should not have hooks
  - there should be a clean separation of mapping from parameterization to theano variable and all the extra (modular) logic
- canopy
  - layer on top of treeano
  - handles non-theano modularity
  - nice reference
    - the only layer above this is the emergent layer (your code)
- add auto-chunking of variables
  - maybe
- function wrappers
  - assert shape of numpy array is same as that of the input variable
- workflows / hooks
  - training
  - distillation
    - input: network1, outputkey1, network2, outputkey2, data generator
  - distillation from caffe model
  - resuming an in-training model
  - optionally resuming an in-training model, or restarting
  - printing progressbar after each epoch
  - checkpoint model every x batches / chunks
  - dynamically changing network during training to things that should be easy in canopy
    - eg. adding additional losses
  - test on validation set every x chunks
  - visualization
    - print filters
    - find most similar inputs to input
      - given:
        - query input
        - possible inputs
        - layer for representation to use
    - given a certain unit, find which inputs fire hardest
    - find inputs that are the most wrong
    - find input that maximally activates for class
      - with constant norm or L1/L2 loss
    - histogram of values of a tensor
      - eg. output of a layer
      - eg. gradient of a weight
    - image visualizations
      - show filter activations for given images
      - show grad(image_class, input)
        - behaves like importance map
      - show input patch that maximally activates a conv neuron
      - deconvolutional networks
    - saving learning curves
    - post-training report
      - eg. examples which are the most wrong
  - monitoring
    - printing out certain values
      - eg. loss
      - eg. average magnitude of gradient
    - alerting on certain conditions
      - eg. nan's or inf's
      - eg. magnitude of gradient/weight matrix too high
    - relationships between variables
  - AUC / accuracy logging
  - saving model
  - logging batch normalization mean/variance updates
  - logging how close rolling BN mean/variance is to actual mean/variance
  - hyperparameter optimization
  - active learning
  - q learning
  - curriculum learning
    - from the data point of view (ie. changing data distribution)
  - experience replay for hard-ish negatives
  - early stopping
  - hyperparameter annealing
    - eg.
      - dropout
      - learning rate
      - the weight of a loss function
  - sliding window application
  - approximate sliding window apply
    - eg. SPPnet
  - attaching preprocessing to a model
    - rationale: so that a serialized model can have preprocessing built in
  - chunking batches together
    - ratioanle: optimized GPU transfer
  - saving last x values of variables
  - auto-stop on nan
  - parameter hot-swapping
  - restarting from previous state w/ lower learning rate
  - proper BN initialization
  - alternating training
    - between several models
  - adversarial training
  - hard negative mining
  - gradual hard negative mining
    - eg. facenet
  - serialization
    - both model & learning states (ie. states of the wrapper functions)
      - thus learning state must be serializable
      - should serialize all inner state
      - need hierarchical state for wrappers/hooks/workflows
  - net surgery
  - easily testing a new node
    - generate inputs / outputs / extra layers (eg. cost/updates/etc.)
  - run model on MNIST/CIFAR/etc.
  - localization
  - detection
  - segmentation
  - cascade of models
    - not necessarily trained jointly (?)
  - test time augmentation
  - ensembling
  - dynamic curriculum learning
    - saving mistaken examples for replaying
    - with metric learning
      - starting with easy values
- blocks callback hooks
  - before_training : bool - is invoked before training.
  - before_first_epoch : bool - is invoked before the first epoch.
  - before_epoch : bool - is invoked before every epoch.
  - on_resumption : bool, optional - is invoked when training is resumed.
  - on_interrupt : bool, optional - is invoked when training is interrupted.
  - after_epoch : bool - is invoked after every epoch.
  - after_batch: bool - is invoked after every batch.
  - after_training : bool - is invoked after training.
  - after_n_epochs : int, optional - if not None, invoked when `after_n_epochs` epochs are done
  - every_n_epochs : int, optional - if not None, invoked after every n-th epoch.
  - after_n_batches : int, optional - if not None, invoked when `after_n_batches` batches are processed.
  - every_n_batches : int, optional - if not None, invoked after every n-th batch.
- datasets
  - RNN
    - Blogger Dataset: http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip (Age and gender data)
      - from https://github.com/IndicoDataSolutions/Passage
    - use MNIST, scan one vector at a time
      - https://github.com/IndicoDataSolutions/Passage/blob/master/examples/mnist.py
- how to save state part-way through training
- seems like a very reasonable assumption that training might involve arbitrary python code execution
  - eg. with deep q learning, might have to simulate a game
- consider hooks
  - implement like clojure handler wrappers?
    - ability to edit givens, updates, etc.
  - would allow alternative workflows
    - eg.
      - curriculum learning
      - adding preprocessing to a model
      - saving plots
      - saving model
  - which hooks?
    - creation of a function
    - training each batch
    - training each chunk
  - question: are hooks local or global
- live plotting/monitoring
  - other tools (blocks / opendeep) use bokeh
- visualization
  - http://deeplearning.net/tutorial/utilities.html
    - code to plot images/filters
