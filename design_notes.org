* general
- architecture as data
  - the architecture of a single model should at every point be serializable and deserializable to the same model
- parameterization should be recursive
  - eg inner parameterization inherit outer parameterization unless explicitly overwritten
    - maybe: parameterization such as update strategy and initialization
  - use this for things like dropout
- shape must be knowable statically, at least for a fraction of the network
  - before learned weight initialization
    - eg. so that fully connected layers know their size
  - not all layers require shape
    - eg. conv/maxpool/spatial pyramid pooling
    - eg. if there is a spatial pyramid pooling layer, don't need to know shapes of conv layers
- all nodes should behave like data and only data
  - the interface should be what data it contains
  - possibly lazy data, though
- serialization
  - model architecture should be serialized in human-readable / edittable format
    - eg. yaml
    - being able to be serialized to json is probably a pretty safe format
      - though not necessarily serialized to json
  - model weights should be in some agnostic key-value store format
    - a similar architecture should be able to read in weights perfectly
    - path -> np.array
  - should be possible to share weights via specifying a similar architecture
    - since the layer names are absolutely critical to what "similar" means, it seems safe to assume that this sharing can come of the form of converting the other net into this key->value form and having the new net simply read from this blob
  - all layers can specify how they should be serialized (as data)
- input parameters
  - assumption: local networks (local = created together / close together) will have similar parameters
  - thus we can have local "inheritance" of parameters
    - eg. outer layer has an immutable dict of parameters, inner layer overwrites them when it has a parameter override, more inner layers can read from parameters with a key
    - this requires non-ambiguous keys
      - ie. 2 layers shouldn't have the same key mean different things
- need metadata on weight parameters
  - still need to specify backprop-able params, since the gradient needs to be calculated
  - still need to specify non-bias params, for regularization?
    - not necessarily, if the layer knows how to handle it's own parameters
    - this makes it easier if there is a large number of layers expected to be handle the same way
    - "the expression problem"
      - you might add a new layer that would want to benefit from all the regularization
      - you might add new regularization that would wnat to benefit from all layers
      - what commonality can we extract from this?
        - different parameters can behave the same way
        - eg. bias / inputweights
          - makes sense to have an "escape hatch" where things can work if not fitting within a mold
  - parameters should have tagging
    - to work with multiple kinds of regularization
- question: should all nodes have a unique name?
  - pros of unique name
    - don't need separate concepts for unique / non-unique
      - and some names do need to be unique (for initialization between networks)
  - cons
    - conceptually unnecessary, as long as they have a unique path
      - eg. don't need to name "convpoolstack_pool" when "pool" has parent "convpoolstack"
        - this would also be DRY-er and make the architectures more human readable
  - conclusion: just go with everything having a unique name
    - this should be easier to understand - and the downsides aren't that bad
- question: should hyperparameters have global default values
  - eg. shared initialization (create the var)
  - eg. weight initialization (0s)
  - answer: see "question: should parameters and shared variables have different initializations?"
- question: should parameters and shared variables have different initializations?
  - pros of different:
    - they are 2 very different things, no reason to have one know about the other
      - eg. Uniform initialization shouldn't have to know how to create a shared variable
    - if they were the same, and there was a subnetwork with different initialization, that subnetwork couldn't be initialized with shared variables from another network
      - this means that there must be a mechanism by which the shared initialization can overwrite that initialization
  - cons of different:
    - this is a purely theano-level concern, thus a conceptual network shouldn't require it
    - when using shared variables from a different network, you probably don't want to re-initialize
  - conclusion:
    - should be different
      - and a weight initialization node should look up the tree for a hyperparameter on shared initialization
    - weight initialization should have a default shared initialization
      - thus no global shared initialization should be needed
    - weight initialization and shared variable creation should happen at the same time, so that weight initialization knows now to overwrite the values in a shared variable
- should lifecycle methods (init_state, init_shared, init_values, and compute) be separate?
  - pros of separation
    - easier to do special schemes like loading existing network
    - may allow input layers to be partially initialized before initializing
  - cons
    - more boilerplate
    - somewhat more elegant to define everything in a single function
      - might not even need laziness
  - misc
    - seems unlikely that there would be a good use for having partially initialized layers
  - thought: init_state is different from the others - since initializing the variables can be lazy
    - alternative: pre_compute and post_compute steps for pre-order and post-order traversal
    - pros of separating only init_state
      - if they happen all at once, some nodes would need a pre-order traversal (eg. an initialization node that creates a stateful initializer), while pretty much all nodes need a post-order traversal (to specify output)
  - conclusion:
    - separate init_state, but have a single compute step which:
      - creates shared variables (parameters)
      - creates outputs
      - sets initial values of shared variables (parameters)
- using networkx to represent the graph
  - pros
    - a bunch of stuff for free
      - make it easy to look for ancestors, etc.
      - topological sort
      - make cute diagram of an architecture
  - cons
    - extra dependency
- should architecture nodes be separate from state nodes?
  - they conceptually serve 2 very different purposes
  - but there implementations are linked together
    - ie. the data in an architecture node is used for a state node
  - separating them would require additional verbosity to combine them together
  - conclusion: keep them together, but make sure not to mutate them
- question: where are dependencies defined?
  - NOTE: only containers need to define dependencies
  - question: how to define a node which is shared between multiple trees (eg. the same thing is processed in 2 places)
    - just because something is build like a tree, doesn't mean there can't be jumps in it
      - can have a "get_layer_with_name" node
  - conclusion:
    - parents automatically depend on their children
    - a "get_layer_with_name" node depends on that layer
    - more dependencies can be defined within nodes
- question: should we be able to add nodes in the graph after build-ing?
  - eg. add special intermediate nodes
  - use case: container node wants to create some sort of state that it's children can depend on
  - conclusion: no
    - one would have to make sure it's initialized just like in build
    - this kind of change seems like it would make it easy to violate assumptions / principle of least surprise
- how to do composite node dependencies properly
  - ie. how to pass input of sequential node to first node
  - how do you have it return the output of its last child? (sequential node)
    - the input of a composite node will be set by its parent - thus it can't set it's own dependencies on its children
  - how to express dependencies of first child:
    - first child depends on input of sequential node
    - last child depends on other children
    - sequential node output depends on last child
    - if first child depends on sequential node, this causes a cycle
  - there is a true dependency between the input of the sequential node and its first child
    - thus it makes sense to make this explicit
  - conclusion:
    - walk tree from top-level to bottom level, so that a dependency between the input of the sequential node and the first child can be set
  - alternative:
    - some form of graph unification
      - eg.
        - unify inputs of sequential node with inputs of first child
        - unify outputs of sequential node with outputs for last child
- question: how are inputs passed into a node?
  - parent node should handle it
- question: how should the input to a node be passed in?
  - options
    - function argument
      - pros
        - more expected
        - can enfore the right arity
      - cons
        - can't specify which key in the output map
    - through the graph
      - eg. self.graph.my_input(some_key="output")
    - through the graph w/ sugar
      - eg. self.get_input(some_key="output")
  - conclusion:
    - through the graph w/ sugar
    - no positional arguments (much harder to specify in a graph)
- why have wrapped variables?
  - additonal metadata
    - eg. shape, tags
  - custom rules
    - eg. related to tags
      - shared variables should be parameters and either weight / bias
      - weight / bias -> also parameter
    - eg. parameters must be shared
  - why make them lazy?
    - original motivation: to allow stage-wise initialization
      - eg.
        - first create the shareds
        - then perform operation with them
        - then adds some values in
  - pros of unwrapped variables
    - can directly look at the computation graph for dependencies
  - solution:
    - include a reference to the wrapped variable in the theano variable
      - so that the computation graph can still be walked for dependencies
- why not using tagging on theano variables?
  - because theano variables can be shared between networks
    - the state of one network shouldn't be messed with the state of another
- question: should nodes update themselves or should a higher level node do it?
  - pros if nodes updates themselves:
    - nodes can make sure each update only occurs once
  - pros of higher level node doing it:
    - code might be cleaner - because it can update all the weights in a single batch
  - question: is there any way to set higher level nodes to only update the weight of a unit once?
    - or can a lower level node override those updates
  - conclusion: higher level node can do it, lower level nodes can override
- how to compute update deltas?
  - compute bottom-up in architectural tree
    - pros:
      - can be done by adding immutable UpdateDeltas together
  - compute top-down in architectural tree
    - so children can overwrite / add
    - pros:
      - more complex logic can happen in a single mode
    - cons:
      - if using a single mutable UpdateDeltas, then inner nodes can see state of nodes that aren't its parents
        - eg.
          - b<-x->a (x w/ children a,b)
          - x computes its updates
          - a can see x's updates
          - thus a can also see updates for node b from x
          - whichever of a,b is computed second will see updates from the other
        - this may cause errors as well if one accidentaly updates weights that they shouldn't be able to see
  - add it all together
    - must be equivalent to an implicit top-down
      - eg. in order for a node to zero out updates to its children, some sort of top-down process must occur
    - pros
      - simpler interface
    - cons
      - complexity must be implemented anyway - why not expose it
  - conclusion: top-down
    - rationale:
      - lower in the tree is more specific, and you'd more likely want to perform an effect on a general setting than apply a more general effect to independent local settings
        - eg. if you want to zero out updates, more likely a non-local update than lots of independent local updates
- how should updatedeltas be updated (assuming top-down / root-to-leaf computation)?
  - it seems that either implicitly or explicitly, update deltas need to be passed into nodes
    - how else would a node zero out the updates?
      - doing so declaratively is a (complex) possibility
        - especially a simplified version with only the ability to add / overwrite
  - options
    - update_deltas should be passed in to a compute_update_deltas function
      - pros
        - consistent with normal python functions
    - update_deltas should be accessed as a attribute of the current node
      - pros
        - makes it clear where they are stored, always being in the same place
        - they should be accessed as an attribute in the future
  - conclusion:
    - update_deltas should be shared for a network
      - because
        - a single network should have a single set of updates
        - every node gets access to the updates anyway via compute_update_deltas
        - the updates should be stored somewhere
          - since they don't change, they can be cached
        - instead of storing then in some arbitrary node, we can store them in all nodes
    - update_deltas should be passed in to a compute_update_deltas function
      - rationale: simpler, makes sense, and might be useful for re-applying/re-ordering updates
      - note: should assert that compute_update_deltas returns None to make the interface clearly be one for mutation
    - because update deltas will be shared and mutable, their api should be made such that it isn't ease to accidentally mess than up
      - eg. use __iadd__ instead of __add__
* best practices
- store all constructor arguments as is
  - don't mutate them
  - don't overwrite them
  - don't forget to store them as an attribute
  - rationale: this allows serialization of the original layer
- use https://github.com/ionelmc/python-fields for constructing nodes
  - helps follow good constructor practices, and provides a nice string representation
  - make sure that the fields super class is defined after the node super class
    - this is so that node methods take precedence
      - this was a issue particularly for __hash__
* assumptions
- shared variables (eg. parameter tensors) will have a fixed size
- nodes are in a directed acyclic graph
  - thus can be traversed in a topological sort
- initialization schemes can work in a topologically sorted order
  - eg. not from output to input
  - not necessarily true, but a simplifying assumption
- shared variables (eg. parameter tensors) are owned by a single node in the computation graph, and that node is responsible for their updates
- node names are strings
- a None value of a hyperparameter signifies that it is missing
- all nodes will have a default output
* random notes
- model everything as a "layer"
  - including loss/objective
  - maybe even represent gradient descent updater as layer node
    - takes in trees of things to update, and loss, and generates updates
  - layer : higher level unit than theano node
    - high level enough that it deserves to both be know and named
  - compose base layers together with functions
    - make sure that functions add a named identity layer
  - use paths instead of strings as names
- issue: how to update an existing architecture to eg. use dropout
  - have a top level assoc_in
    - to add new parameters or replace layers
- responsibilities of each "layer"
  - serialization
  - deserialization
  - output(s)
    - default key for output: "output"
    - map w/ names
    - rationale for multiple outputs:
      - monitoring is one possible use case
  - shape(s)
    - optional: can auto-compute
  - update(s)
    - optional
  - dimension(s)
    - maybe?
- everything has names and paths
  - each node needs a name
  - names are like relative paths
  - paths are absolute in the network
    - tuples of names / indexes
- sharing shared: separate step for initializing shared
  - pass in a map from path to shared, which can be used instead of creating a new shared variable
- sharing weights: separate step for initializing weights
  - pass in a map from path to weights, which can be used instead of creating a new shared variable
- sharing params:
  - use a network.assoc_in(path, value)
    - eg. top_level_network.assoc_in(["deterministic"], True)
- question: when are shareds / weights initialized
  - initializing shared
    - prereq: dimensionality, broadcast dims
  - initializing weights
    - prereq: shape
- state in a node is divide into:
  - hyperparameters
  - shared variables
  - intermediates
- how to do shared initialization:
  - kinds of shared initialization
    - exact path initialization
      - eg. ("FCs", "head", 1, "FC")
    - partial path initialization
      - eg. ("some_initalization", "something", "my_cool_layer", "FC") equivalent to ("my_cool_layer", "FC")
  - question: conceptually, how does one perform initialization with existing shareds
    - SharedInitialization.fromNode(other_network, backup_initialization=NoInitialization())
      - set default to having no init
* features
- modularity
  - kind of a design goal of the other features
  - "tricks" can be made 100% self-contained
- hierarichical specification of hyperparameters
  - customize each subnetwork
- nodes can update themselves
- storing updates as changes in the values instead of new values
  - allow arithmetic on the amount changes
* planned features
- serialization
- easy variable sharing between networks
- easy weight sharing between networks
- auto-"chunk"-ing theano variables
- automatically computing shape of node outputs
- graph transformations
  - eg. give me a new network with all ReLUs substituted for leaky ReLUs
- recurrent networks
* TODOs
- use an exception for not having a hyperparameter set instead of None
- use assertions for outputs that should not be used
- redo node impl
  - node impl: provides sane default functionality - that can easily be overwritten
    - hash, str, repr
    - assert there is a name
    - return update deltas that are added in to previous update deltas
    - build shortcut
  - have base node class have a minimal API with no defaults
    - get_hyperparameter
    - compute_update_deltas
    - to_architecture_data
    - from_architecture_data
    - architecture_children
    - TODO
      - init_state
      - compute_output
  - add as functions
    - architecture_copy
    - children_names
  - transfer functionality to network : maybe merge with "graph"
    - build
    - __getitem__
    - function
    - compute_all_update_deltas
    - create_variable
  - create partially applied network that is passed into nodes that allows for easy creation of state
- theano.scan(strict=True)
  - problem: issue with passing in random variables
  - options
    - ignore and use strict=False
    - have a flag to ensure strict=True
      - somehow verify that the graph has no random variables
- how to handle updates for RandomStates?
  - options:
    - ignore it and have default_updates handle it
      - it would have to be explicitly ignored by scan
    - add it to the updates
      - problem: what if there are multiple scans - adding the updates don't make sense in this case
      - pros:
        - easier
- when adding updates together, it might cause overflows / auto-promotion
  - TODO make sure to recast into original dtype when combining updates
- workflows / hooks
  - distillation
    - input: network1, outputkey1, network2, outputkey2, data generator
  - resuming an in-training model
  - optionally resuming an in-training model, or restarting
  - printing progressbar after each epoch
  - checkpoint model every x batches / chunks
  - test on validation set every x chunks
  - visualization
    - print filters
    - find most similar inputs to input
      - given:
        - query input
        - possible inputs
        - layer for representation to use
    - given a certain unit, find which inputs fire hardest
    - find inputs that are the most wrong
    - find input that maximally activates for class
      - with constant norm or L1/L2 loss
    - histogram of values of a tensor
      - eg. output of a layer
      - eg. gradient of a weight
    - image visualizations
      - show filter activations for given images
      - show grad(image_class, input)
        - behaves like importance map
      - show input patch that maximally activates a conv neuron
      - deconvolutional networks
  - monitoring
    - printing out certain values
      - eg. loss
      - eg. average magnitude of gradient
    - alerting on certain conditions
      - eg. nan's or inf's
      - eg. magnitude of gradient/weight matrix too high
- blocks callback hooks
    before_training : bool
        If ``True``, :meth:`do` is invoked before training.
    before_first_epoch : bool
        If ``True``, :meth:`do` is invoked before the first epoch.
    before_epoch : bool
        If ``True``, :meth:`do` is invoked before every epoch.
    on_resumption : bool, optional
        If ``True``, :meth:`do` is invoked when training is resumed.
    on_interrupt : bool, optional
        If ``True``, :meth:`do` is invoked when training is interrupted.
    after_epoch : bool
        If ``True``, :meth:`do` is invoked after every epoch.
    after_batch: bool
        If ``True``, :meth:`do` is invoked after every batch.
    after_training : bool
        If ``True``, :meth:`do` is invoked after training.
    after_n_epochs : int, optional
        If not ``None``, :meth:`do` is invoked when `after_n_epochs`
        epochs are done.
    every_n_epochs : int, optional
        If not ``None``, :meth:`do` is invoked after every n-th epoch.
    after_n_batches : int, optional
        If not ``None``, :meth:`do` is invoked when `after_n_batches`
        batches are processed.
    every_n_batches : int, optional
        If not ``None``, :meth:`do` is invoked after every n-th batch.
- adding to updates vs adding to cost
  - adding to updates is definitely necessary
    - because some state is just updated
      - eg. rolling mean/variance
  - adding to cost
    - pros
      - if you want to use the same learning algorithm (eg. Adam), it is more convenient
    - cons
      - might be confusing to have 2 ways to do this
    - how?
      - options
        - add to a global cost
        - don't automatically add to costs
          - thus need to manually add to costs
        - have the ability to query for cost of a subtree
- canopy
  - layer on top of treeano
  - handles non-theano modularity
  - nice reference
    - the only layer above this is the emergent layer (your code)
  - naming
    - should this be mixed with treeano? if so, should the core stuff be treeano.core or treeano.understory?
- have nodes take in a network state instead of changing node state
- a bunch of the things that seem like lasagne annoyances are actually for good reason
  - ie. get_output_for to allow calculating output w.r.t. anything
    - it behaves more like a function
- how to save state part-way through training
- seems like a very reasonable assumption that training might involve arbitrary python code execution
  - eg. with deep q learning, might have to simulate a game
- consider hooks
  - implement like clojure handler wrappers?
    - ability to edit givens, updates, etc.
  - would allow alternative workflows
    - eg.
      - curriculum learning
      - adding preprocessing to a model
      - saving plots
      - saving model
  - which hooks?
    - creation of a function
    - training each batch
    - training each chunk
  - question: are hooks local or global
- treeano should not have hooks
  - there should be a clean separation of mapping from parameterization to theano variable and all the extra (modular) logic
- consider linking independent functions together in class
  class Foo:
    something1 = some_fn
    something2 = other_fn
- PYTHONPATH=~/repos/treeano:$PYTHONPATH sniffer
- grep for FIXME's
  - separate out lasagne initializations from initializations
- sugar:
  - some node mixin for defining hyperparameters
  - some single node wrapper mixin
  - have build return a Network object that has network specific features
    - eg. __getitem__, function
- architecture nodes ONLY have 2 things: hyperparameters and children
  - in various desired formats
    - might want custom ways of inputting children
      - eg. networkx DAG
    - might want custom ways of inputting hyperparameters
      - eg. hyperparameter node
        - takes in kwargs
      - eg. something that provides stateful hyperparameters
  - how to effectively define these things in an easy to use way?
    - to minimize overall work + maximize simplicity
      - specifically in:
        - __init__
        - getting hyperparameters
        - getting children
        - serialization
  - how to define hyperparameters and children?
- define output computation as function from inputs to dict of outputs
  - not a staticmethod, because some transformations require object state
    - eg. SPPnet layer requires shapes
- scratchpad
  - temperaturesoftmaxnode
  - temperature softmax + categorical cross entropy + rescaling due to temperature (* T^2)
  - gradient caching node
    - allows calculating derivatives of parameters with respect to loss high up in the tree (optimization)
  - learning rate decay node
    - represents learning rate parameter as theano var w/ updates
  - javascript interface to expand nodes
    - more expanded = more low-level
  - allow input nodes to have default values (theano.param)
- remove python-fields
  - rationale:
    - want to have logic in constructor?
    - only benefit is __repr__, which can be provided by Node
- NOTE: hyperparameters are never accessed directly, because of propagation - should always use find_hyperparameter
  - perhaps we can use some syntactic sugar to do an auto-search
    - override __getattribute__ for hyperparameters
    - override __setattribute__ to not work on hyperparameters
    - some sugar to specify defaults as well?
- add auto-chunking of variables
  - maybe
- try dropout network w/ train / test
  - hyperparameter: have default of enable_dropout be deterministic
- function to wrap lasagne layer boilerplate
  - input:
    - lazy wrapped variable
    - layer constructor
    - kwargs for constructor
  - output: lazy wrapped variable
- test:
  - find_variables
  - preallocatedinitialization
  - theano.tag.test_value
    - http://deeplearning.net/software/theano/tutorial/debug_faq.html
- easy way of accessing nodes in network
  - eg. network.get_node("node_name")
  - useful after building a network and wanting to inspect state
- real initialization nodes
  - using a stateful class and a hyperparameternode is very wrong
    - just a hack
  - want to represent the parameters of the initialization as hyperparameters and have them be clone-able/inspect-able
- PreallocatedInitializationNode
  - sets hyperparameters:
    - shared_initializations
    - preallocated_initialization
  - rationale for setting both:
    - if no initialization scheme is given, then it uses the former
    - if another initialization scheme is given within the network, that scheme still has the choice of finding a preallocated_initialization hyperparameter and prioritizing that
    - this allows no hard-coded preference to preallocation in the core code
- fix architecture to data / from data / copy
  - problem: what if node has children in its state
    - some sort of walk needs to occur converting everything into data and back
- wrap lasagne
  - update code
  - initialization
  - layers
- custom wrapper around variables
  - if shape is not given, print warning and calculate
    - theano.function([x, y], [x], on_unused_input="ignore")(2, 3)
  - walk the theano graph:
    - var.owner.inputs
      - var.owner is None when var is not derived
    - http://deeplearning.net/software/theano/tutorial/symbolic_graphs.html
- create graph transformations
  - eg.
    - substitution
      - eg. replace ReLU -> PReLU / leaky ReLU
        - use an update-in operation on the architecture as data that does a tree walk
      - to substitute in only a subtree
    - adding different initialization
- break up into different modules
- make insides of update algorithms (eg. intermediates in nesterov momentum inspectable)
- create pretty printing of nodes
  - like how theano prints it's computation graph
- demos
  - vanilla mnist
  - fully connected layer mnist with num_hidden, activation specified once
  - separate conv layers from FC layers
    - different dropout and L2 for conv
- how to do things guide: (thinking in treeano guide)
  - freezing a section
    - use UpdateScaleNode, scale to 0
- use class decorator for registering classes for serialization
  - look into using a metaclass in the future
- consider as a name: treehano
  - pros
    - sounds cooler
  - cons
    - sounds less like theano
- issue: recurrent nets
  - if everything is kept as a function from parameters -> theano var, it should compose well with recurrent nets
  - recurrent layers would probably behave something like a multiple input layer
  - if a net is recurrent, this seems like it would just take an orthogonal compilation step to convert the resulting net to use scan
  - problem: we work with variables, not functions
    - solution given input variables and an output variable, we should be able to construct a version of an output variable by doing a walk on the transitive closure of the output variable and replace the inputs
      - thus we can get functions from variables
  - make a symbolic_function
    - input:
      - inputs: variables [x, ...] (maybe computed variables)
      - outputs: variables [y, ...]
    - output:
      - function that takes in replacements for inputs, and returns new outputs for those inputs
    - how?
      - take in new inputs
      - perform a deepcopy on the outputs
      - walk both the old and new outputs at the same time
      - when a variable is in the old outputs from the old inputs, replace it with one of the new inputs
- look into drawing:
  - https://github.com/Lasagne/Lasagne/issues/174
    - https://github.com/ebenolson/Lasagne/blob/master/examples/draw_net.py
- live plotting/monitoring
  - other tools (blocks / opendeep) use bokeh
- look into attentional interfaces
  - https://github.com/bartvm/blocks/blob/master/blocks/bricks/attention.py
- datasets
  - RNN
    - Blogger Dataset: http://www.cs.biu.ac.il/~koppel/blogs/blogs.zip (Age and gender data)
      - from https://github.com/IndicoDataSolutions/Passage
    - use MNIST, scan one vector at a time
      - https://github.com/IndicoDataSolutions/Passage/blob/master/examples/mnist.py
